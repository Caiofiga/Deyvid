{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INGESTÃO DE DADOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "'''\n",
    "Nada do que esta escrito nessa secao foi descrito na Biblia. Oficialmente fomos abandonados por Deus.\n",
    "\n",
    "Esse dataset eh grande pra caralho, e ninguem no mundo tem memoria suficiente pra processar tudo de uma vez.\n",
    "tipo eu to falando de 200GB de memoria RAM, vai se fuder mano, ninguem tem isso.\n",
    "Detalhe, descobri essa noticia depois de 15min do computador trabalhando. Imagina ficar sentado na frente de uma tela soh olhado\n",
    "o computador carregar, rezando a Deus que desse certo. Nao deu certo.\n",
    "primeiramente, eu tentei separar em pedacos, soh usando os chunks e trabalhando com 7 for loops por vez, um pra cada planilha\n",
    "melhorou, agora agnt precisava de 100GB de memoria RAM. Mesmo assim, vai se fuder \n",
    "Depois eu pensei (eu neh, nessa hora a unica pessoa que tinha saco pra fazer essa bosta era o ChatGPT) em fazer um por vez,\n",
    "cagando para os chunks e so fazendo um atras do outro. Claro, nao funcionou.\n",
    "Entao, eu, ja de saco cheio com esse dataset de bosta, taquei o fodasse e soh pedi pro Gepeto resolver pra mim o problema\n",
    "quando ele me manda a mesma MERDA que eu tinha feito, soh que com a porra dos chunks, eu oficialmente desisti dessa bosta\n",
    "Obrigado a todos pela Compreensao.\n",
    "\n",
    "PS: Vai se fuder administracao.\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "# Define chunk size for processing large datasets\n",
    "chunk_size = 1000\n",
    "\n",
    "# Define file paths and columns to use\n",
    "file_paths = {\n",
    "    'admissions': 'csvs/ADMISSIONS.csv',\n",
    "    'microbiology_events': 'csvs/MICROBIOLOGYEVENTS.csv',\n",
    "    'prescriptions': 'csvs/PRESCRIPTIONS.csv',\n",
    "    'labevents': 'csvs/LABEVENTS.csv',\n",
    "    'diagnoses_icd': 'csvs/DIAGNOSES_ICD.csv',\n",
    "    'procedures_icd': 'csvs/PROCEDURES_ICD.csv',\n",
    "    'patients': 'csvs/PATIENTS.csv',\n",
    "    'icustays': 'csvs/ICUSTAYS.csv'\n",
    "}\n",
    "\n",
    "cols_to_use = {\n",
    "    'admissions': ['subject_id', 'hadm_id' ,'admittime', 'dischtime', 'ethnicity', 'admission_type'],\n",
    "    'microbiology_events': ['row_id', 'subject_id', 'hadm_id', 'chartdate', 'charttime', 'spec_itemid', 'spec_type_desc', \n",
    "                            'org_itemid', 'org_name', 'isolate_num', 'ab_itemid', 'ab_name', 'dilution_text', \n",
    "                            'dilution_comparison', 'dilution_value', 'interpretation'],\n",
    "    'prescriptions': ['subject_id', \"hadm_id\", 'drug', 'enddate'],\n",
    "    'labevents': ['subject_id', 'itemid', 'hadm_id','valuenum', 'valueuom', 'flag'],\n",
    "    'diagnoses_icd': ['subject_id', 'icd9_code', 'hadm_id', 'seq_num'],\n",
    "    'procedures_icd': ['subject_id', 'icd9_code',  'hadm_id','seq_num'],\n",
    "    'patients': ['subject_id', 'gender', 'dob' ,'dod'],\n",
    "    'icustays': ['subject_id', 'los', 'first_careunit', 'hadm_id' ,'last_careunit']\n",
    "}\n",
    "\n",
    "# Specify dtype for memory optimization\n",
    "dtype_spec = {\n",
    "    'subject_id': 'Int32',  # Nullable integer type\n",
    "    'hadm_id': 'Int32',     # Nullable integer type\n",
    "    'itemid': 'Int32',\n",
    "    'valuenum': 'float32'    # Floats can have NaN, so no issues here\n",
    "}\n",
    "\n",
    "# Helper function to merge two datasets chunk by chunk and write to a file\n",
    "def merge_and_save(existing_df, new_df_chunks, on='subject_id', suffixes=('', '_new'), output_file='intermediate.csv'):\n",
    "    \"\"\"\n",
    "    Merges an existing DataFrame with a new one (loaded in chunks) and writes the result to a file.\n",
    "    \"\"\"\n",
    "    # Process each chunk of the new DataFrame\n",
    "    for new_chunk in new_df_chunks:\n",
    "        # Merge the current chunk with the existing DataFrame\n",
    "        merged_chunk = pd.merge(existing_df, new_chunk, on=on, how='outer', suffixes=suffixes)\n",
    "        \n",
    "        # Save the merged chunk to the output file\n",
    "        merged_chunk.to_csv(output_file, mode='a', header=False, index=False)\n",
    "\n",
    "        # Optional: Delete the merged chunk from memory after writing to disk\n",
    "        del merged_chunk\n",
    "\n",
    "# Define the output file for the final merged result\n",
    "output_file = 'PELO_AMOR_DE_DEUS_NAO_DELETA_ESSA_PORRA.csv'\n",
    "\n",
    "# Step 1: Start with the first dataset (e.g., microbiology_events)\n",
    "for micro_chunk in pd.read_csv(file_paths['microbiology_events'], usecols=cols_to_use['microbiology_events'], dtype=dtype_spec, chunksize=chunk_size):\n",
    "    # Save the first chunk to initialize the output file\n",
    "    micro_chunk.to_csv(output_file, mode='w', header=True, index=False)\n",
    "\n",
    "    # Step 2: Merge with each subsequent dataset one at a time\n",
    "\n",
    "    # Merge with prescriptions\n",
    "    pres_chunks = pd.read_csv(file_paths['prescriptions'], usecols=cols_to_use['prescriptions'], dtype=dtype_spec, chunksize=chunk_size)\n",
    "    merge_and_save(micro_chunk, pres_chunks, on='subject_id', suffixes=('', '_pres'), output_file=output_file)\n",
    "\n",
    "    # Merge with admissions\n",
    "    adm_chunks = pd.read_csv(file_paths['admissions'], usecols=cols_to_use['admissions'], dtype=dtype_spec, chunksize=chunk_size)\n",
    "    merge_and_save(micro_chunk, adm_chunks, on='subject_id', suffixes=('', '_adm'), output_file=output_file)\n",
    "\n",
    "    # Merge with diagnoses_icd\n",
    "    diag_chunks = pd.read_csv(file_paths['diagnoses_icd'], usecols=cols_to_use['diagnoses_icd'], dtype=dtype_spec, chunksize=chunk_size)\n",
    "    merge_and_save(micro_chunk, diag_chunks, on='subject_id', suffixes=('', '_diag'), output_file=output_file)\n",
    "\n",
    "    # Merge with labevents\n",
    "    lab_chunks = pd.read_csv(file_paths['labevents'], usecols=cols_to_use['labevents'], dtype=dtype_spec, chunksize=chunk_size)\n",
    "    merge_and_save(micro_chunk, lab_chunks, on='subject_id', suffixes=('', '_lab'), output_file=output_file)\n",
    "\n",
    "    # Merge with procedures_icd\n",
    "    proc_chunks = pd.read_csv(file_paths['procedures_icd'], usecols=cols_to_use['procedures_icd'], dtype=dtype_spec, chunksize=chunk_size)\n",
    "    merge_and_save(micro_chunk, proc_chunks, on='subject_id', suffixes=('', '_proc'), output_file=output_file)\n",
    "\n",
    "    # Merge with patients\n",
    "    pat_chunks = pd.read_csv(file_paths['patients'], usecols=cols_to_use['patients'], dtype=dtype_spec, chunksize=chunk_size)\n",
    "    merge_and_save(micro_chunk, pat_chunks, on='subject_id', suffixes=('', '_pat'), output_file=output_file)\n",
    "\n",
    "    # Merge with icustays\n",
    "    icu_chunks = pd.read_csv(file_paths['icustays'], usecols=cols_to_use['icustays'], dtype=dtype_spec, chunksize=chunk_size)\n",
    "    merge_and_save(micro_chunk, icu_chunks, on='subject_id', suffixes=('', '_icu'), output_file=output_file)\n",
    "\n",
    "    # After each step, the merged data is saved in `final_merged_output.csv`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TREINO DO MODELO PREDITIVO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_recommenders as trfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SUGESTAO "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drug_suggestion(org_name, resistance):\n",
    "    if resistance == 'R':\n",
    "        return \"Sugestão de antibiótico alternativo\"\n",
    "    else:\n",
    "        return \"Sugestão de antibiótico padrão\"\n",
    "\n",
    "merged_data['sugestao'] = merged_data.apply(lambda x: drug_suggestion(x['org_name'], x['interpretation']), axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FERRAMENTA VALIDAÇÃO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Testar a ferramenta em um conjunto de dados de validação\n",
    "resultados_reais = merged_data1['antibiotico_real']\n",
    "sugestoes = merged_data1['sugestao']\n",
    "\n",
    "acuracia = accuracy_score(resultados_reais, sugestoes)\n",
    "print(f\"Acurácia da ferramenta: {acuracia}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
