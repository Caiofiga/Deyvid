{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ingestão de dados\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "#criando dicionário para facilitar o nome dos arquivos \n",
    "file_paths = {\n",
    "    'admissions': 'rawcsvs/ADMISSIONS.csv',\n",
    "    'microbiology_events': 'rawcsvs/MICROBIOLOGYEVENTS.csv',\n",
    "    'prescriptions': 'rawcsvs/PRESCRIPTIONS.csv',\n",
    "    'labevents': 'rawcsvs/LABEVENTS.csv',\n",
    "    'diagnoses_icd': 'rawcsvs/DIAGNOSES_ICD.csv',\n",
    "    'patients': 'rawcsvs/PATIENTS.csv',\n",
    "    'transl-labitems': 'rawcsvs/D_LABITEMS.csv',\n",
    "    'transl-diag': 'rawcsvs/D_ICD_DIAGNOSES.csv'\n",
    "}\n",
    "\n",
    "\n",
    "#carregando arquivos\n",
    "files = {}\n",
    "for file_path in file_paths:\n",
    "    files[file_path] = pd.read_csv(file_paths[file_path])\n",
    "    \n",
    "for file_path, file in files.items(): \n",
    "    if  not file_path.startswith('transl'):\n",
    "        file.dropna(inplace=True, subset=['subject_id'])\n",
    "        file.drop_duplicates(inplace=True)  \n",
    "        file_name = os.path.basename(file_path)\n",
    "        file.to_csv(f'cleanedcsv/{file_name}.csv', index=False) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Criando o dataset para o Modelo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lendo os dados dos arrquivos CSV\n",
    "patients = pd.read_csv(os.path.join('rawcsvs', 'PATIENTS.csv'))\n",
    "admissions = pd.read_csv(os.path.join('rawcsvs', 'ADMISSIONS.csv'))\n",
    "microbiology = pd.read_csv(os.path.join('rawcsvs', 'MICROBIOLOGYEVENTS.csv'))\n",
    "\n",
    "# Mesclando os dados de pacientes e admissões em um novo DataFrame chamado new_data\n",
    "new_data = (admissions\n",
    "            .filter(['subject_id', 'admission_type', 'ethnicity', 'diagnosis', 'hospital_expire_flag', 'insurance', 'religion', 'marital_status'])\n",
    "            .merge(patients, on='subject_id', how='left')\n",
    "            )\n",
    "\n",
    "# Mesclando os dados de microbiologia e aplicando one-hot encoding na coluna 'interpretation'\n",
    "new_data = (\n",
    "    microbiology\n",
    "    .filter(['subject_id', 'ab_name', 'interpretation'])\n",
    "    # One hot encoding the interpretation and overwriting the original 'interpretation' column\n",
    "    .assign(interpretation=(microbiology['interpretation'] == 'R').astype(int))\n",
    "    .merge(new_data, on='subject_id', how='left')\n",
    ")\n",
    "\n",
    "# Transformando os diagnósticos em formato 'wide' (largura)\n",
    "aux1 = (\n",
    "    new_data\n",
    "    .filter(['subject_id', 'diagnosis'])\n",
    "    .assign(value=1)\n",
    "    .pivot_table(index='subject_id', columns='diagnosis', values='value', fill_value=0)\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Agrupando as colunas restantes por 'subject_id'\n",
    "aux2 = (\n",
    "    new_data\n",
    "    .groupby('subject_id')\n",
    "    .agg({\n",
    "        'insurance': 'first',\n",
    "        'religion': 'first',\n",
    "        'marital_status': 'first',\n",
    "        'ethnicity': 'first',\n",
    "        'gender': 'first',\n",
    "        'ab_name': 'first',\n",
    "        'interpretation': 'first'  # Now this is the one-hot encoded interpretation\n",
    "    })\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Mesclando aux1 e aux2 para o data set final em um data frame\n",
    "filtered_data = aux1.merge(aux2, on='subject_id', how='left')\n",
    "\n",
    "# Exportando os dados filtrados para o CSV\n",
    "filtered_data.to_csv('improving.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Engenharia de Features (Feature Engineering)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  insurance  religion marital_status               ethnicity gender  \\\n",
      "0  Medicare  CATHOLIC      SEPARATED  BLACK/AFRICAN AMERICAN      F   \n",
      "1   Private  CATHOLIC         SINGLE   UNKNOWN/NOT SPECIFIED      F   \n",
      "2  Medicare  CATHOLIC            NaN   UNKNOWN/NOT SPECIFIED      F   \n",
      "3  Medicare  CATHOLIC       DIVORCED                   WHITE      F   \n",
      "4  Medicare  CATHOLIC       DIVORCED                   WHITE      M   \n",
      "\n",
      "        ab_name  interpretation  Cardiovascular  Respiratory  Neurological  \\\n",
      "0    VANCOMYCIN               0             0.0          0.0           0.0   \n",
      "1           NaN               0             0.0          0.0           0.0   \n",
      "2           NaN               0             0.0          0.0           0.0   \n",
      "3           NaN               0             0.0          0.0           0.0   \n",
      "4  ERYTHROMYCIN               1             0.0          0.0           0.0   \n",
      "\n",
      "   Infectious  Gastrointestinal  Renal  Endocrine  Oncology  Trauma  Other  \n",
      "0         1.0               0.0    0.0        0.0       0.0     0.0    0.0  \n",
      "1         0.0               1.0    0.0        0.0       0.0     0.0    0.0  \n",
      "2         1.0               0.0    0.0        0.0       0.0     0.0    0.0  \n",
      "3         0.0               0.0    0.0        0.0       0.0     1.0    0.0  \n",
      "4         0.0               1.0    0.0        0.0       0.0     0.0    0.0  \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Carrega o arquivo CSV 'improving.csv' em um DataFrame chamado filtered_data\n",
    "filtered_data = pd.read_csv('improving.csv', sep=',')\n",
    "\n",
    "# Passo 1: Identifica as colunas que não são relacionadas a doenças\n",
    "non_disease_columns = ['subject_id', 'insurance', 'religion', 'marital_status', 'ethnicity', 'gender', 'ab_name', 'interpretation'] \n",
    "# Cria uma lista de todas as colunas que são doenças (excluindo as colunas não relacionadas a doenças)\n",
    "disease_columns = [col for col in filtered_data.columns if col not in non_disease_columns]\n",
    "\n",
    "# Passo 2: Mapeia cada doença a seus tipos \n",
    "# Define um dicionário que mapeia categorias de doenças (ex: Cardiovascular, Respiratória, etc.) a listas de doenças específicas\n",
    "disease_type_mapping = {\n",
    "    'Cardiovascular': ['ACUTE PULMONARY EMBOLISM', 'CONGESTIVE HEART FAILURE', 'CORONARY ARTERY DISEASE', 'STEMI', 'MI CHF', 'BRADYCARDIA', 'VF ARREST', 'PERICARDIAL EFFUSION', 'PULMONARY EDEMA, MI', 'PULMONARY EDEMA\\\\CATH', 'CRITICAL AORTIC STENOSIS/HYPOTENSION', 'INFERIOR MYOCARDIAL INFARCTION\\\\CATH'],\n",
    "    'Respiratory': ['ACUTE RESPIRATORY DISTRESS SYNDROME', 'ASTHMA', 'COPD FLARE', 'PNEUMONIA', 'RESPIRATORY DISTRESS', 'SHORTNESS OF BREATH', 'HYPOXIA', 'TACHYPNEA', 'PLEURAL EFFUSION', 'MEDIASTINAL ADENOPATHY'],\n",
    "    'Neurological': ['ALTERED MENTAL STATUS', 'SEIZURE', 'STATUS EPILEPTICUS', 'STROKE/TIA', 'INTRACRANIAL HEMORRHAGE', 'HEADACHE', 'FACIAL NUMBNESS', 'BASAL GANGLIN BLEED', 'ALTERED MENTAL STATUS'],\n",
    "    'Infectious': ['CELLULITIS', 'FEVER', 'SEPSIS', 'URINARY TRACT INFECTION', 'UROSEPSIS', 'PYELONEPHRITIS', 'UTI', 'HIV', 'ABSCESS', 'PNEUMONIA;TELEMETRY', 'SEPSIS; UTI', 'FEVER;URINARY TRACT INFECTION'],\n",
    "    'Gastrointestinal': ['ABDOMINAL PAIN', 'GASTROINTESTINAL BLEED', 'LOWER GI BLEED', 'UPPER GI BLEED', 'VARICEAL BLEED', 'HEPATITIS', 'LIVER FAILURE', 'CHOLECYSTITIS', 'CHOLANGITIS', 'PANCREATITIS', 'ACUTE CHOLECYSTITIS', 'ACUTE CHOLANGITIS', 'HEPATITIS B', 'ALCOHOLIC HEPATITIS', 'ELEVATED LIVER FUNCTIONS;S/P LIVER TRANSPLANT', 'FAILURE TO THRIVE'],\n",
    "    'Renal': ['ACUTE RENAL FAILURE', 'CHRONIC KIDNEY DISEASE', 'RENAL CANCER', 'HYPOTENSION, RENAL FAILURE'],\n",
    "    'Endocrine': ['DIABETES', 'HYPOGLYCEMIA', 'HYPERGLYCEMIA', 'HYPONATREMIA'],\n",
    "    'Oncology': ['LUNG CANCER', 'BREAST CANCER', 'COLON CANCER', 'LEUKEMIA', 'LYMPHOMA', 'METASTATIC MELANOMA', 'NON SMALL CELL CANCER', 'METASTIC MELANOMA;ANEMIA', 'CHRONIC MYELOGENOUS LEUKEMIA;TRANSFUSION REACTION'],\n",
    "    'Trauma': ['FRACTURE', 'MOTOR VEHICLE ACCIDENT', 'SUBDURAL HEMATOMA', 'LEFT HIP FRACTURE', 'RIGHT HUMEROUS FRACTURE', 'HUMERAL FRACTURE', 'S/P MOTOR VEHICLE ACCIDENT', 'ACUTE SUBDURAL HEMATOMA'],\n",
    "    'Other': ['OVERDOSE', 'FAILURE TO THRIVE', 'MEDIASTINAL ADENOPATHY', 'ARREST', 'OVERDOSE', 'AROMEGLEY;BURKITTS LYMPHOMA', 'TRACHEAL ESOPHAGEAL FISTULA', 'TRACHEAL STENOSIS', 'VOLVULUS', 'ANEMIA', 'BURKITTS LYMPHOMA', 'NON SMALL CELL CANCER;HYPOXIA', 'METASTATIC MELANOMA;BRAIN METASTASIS']\n",
    "}\n",
    "\n",
    "# Inicializa um dicionário para mapear cada coluna de doença para seu(s) tipo(s) de doença\n",
    "disease_to_type = {}\n",
    "\n",
    "# Processa cada coluna de doença\n",
    "for col in disease_columns:\n",
    "    # Separa os nomes de doenças caso contenham múltiplas doenças\n",
    "    diseases = re.split(';|,|/', col)\n",
    "    diseases = [disease.strip().upper() for disease in diseases]\n",
    "    types = set()\n",
    "    for disease in diseases:\n",
    "         # Encontra o tipo de doença correspondente\n",
    "        found = False\n",
    "        for disease_type, disease_list in disease_type_mapping.items():\n",
    "            if disease in disease_list:\n",
    "                types.add(disease_type)\n",
    "                found = True\n",
    "        if not found:\n",
    "            types.add('Other')  #  Se a doença não for encontrada, classifica como 'Other'\n",
    "    disease_to_type[col] = types # Mapeia a coluna da doença para os tipos encontrados\n",
    "\n",
    "\n",
    "# Passo 3: Para cada tipo de doença, soma as colunas correspondentes de doenças\n",
    "for disease_type in disease_type_mapping.keys():\n",
    "    # Obtém todas as colunas que correspondem a esse tipo de doença\n",
    "    cols = [col for col, types in disease_to_type.items() if disease_type in types]\n",
    "    if cols:\n",
    "        # Soma as colunas (como elas são binárias, isso conta o número de doenças desse tipo)\n",
    "        filtered_data[disease_type] = filtered_data[cols].sum(axis=1)\n",
    "    else:\n",
    "        filtered_data[disease_type] = 0  # Se não houver doenças desse tipo, atribui 0\n",
    "\n",
    "# Remove as colunas originais de doenças do DataFrame\n",
    "filtered_data.drop(columns=disease_columns, inplace=True)\n",
    "# Remove a coluna 'subject_id', pois não será mais necessária para as análises a seguir\n",
    "filtered_data.drop(columns=['subject_id'], inplace=True)    \n",
    "\n",
    "# Salva o DataFrame filtrado em um novo arquivo CSV\n",
    "filtered_data.to_csv('improving2.csv', index=False)\n",
    "\n",
    "# o DataFrame filtered_data contém as colunas não relacionadas a doenças e as colunas somadas por tipo de doença\n",
    "print(filtered_data.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Treinando e Testando o Modelo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Random Forest Results:\n",
      "Accuracy: 0.93\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.96      0.96        27\n",
      "           1       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.93        28\n",
      "   macro avg       0.48      0.48      0.48        28\n",
      "weighted avg       0.93      0.93      0.93        28\n",
      "\n",
      "\n",
      "Logistic Regression Results:\n",
      "Accuracy: 0.89\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.89      0.94        27\n",
      "           1       0.25      1.00      0.40         1\n",
      "\n",
      "    accuracy                           0.89        28\n",
      "   macro avg       0.62      0.94      0.67        28\n",
      "weighted avg       0.97      0.89      0.92        28\n",
      "\n",
      "\n",
      "SVM Results:\n",
      "Accuracy: 0.93\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.93      0.96        27\n",
      "           1       0.33      1.00      0.50         1\n",
      "\n",
      "    accuracy                           0.93        28\n",
      "   macro avg       0.67      0.96      0.73        28\n",
      "weighted avg       0.98      0.93      0.95        28\n",
      "\n",
      "\n",
      "Gradient Boosting Results:\n",
      "Accuracy: 0.89\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.93      0.94        27\n",
      "           1       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.89        28\n",
      "   macro avg       0.48      0.46      0.47        28\n",
      "weighted avg       0.93      0.89      0.91        28\n",
      "\n",
      "\n",
      "KNN Results:\n",
      "Accuracy: 0.89\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.93      0.94        27\n",
      "           1       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.89        28\n",
      "   macro avg       0.48      0.46      0.47        28\n",
      "weighted avg       0.93      0.89      0.91        28\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "\n",
    "df = pd.read_csv('improving2.csv')\n",
    "\n",
    "#  Remove linhas onde o alvo 'interpretation' está ausente (NaN)\n",
    "df.dropna(subset=['interpretation'], inplace=True)\n",
    "\n",
    "# Seleciona as colunas de características relevantes e a coluna alvo ('interpretation')\n",
    "features = ['insurance', 'religion', 'marital_status', 'ethnicity', 'gender', 'ab_name',\n",
    "            'Cardiovascular', 'Respiratory', 'Neurological', 'Infectious', 'Gastrointestinal', 'Renal',\n",
    "            'Endocrine', 'Oncology', 'Trauma', 'Other']\n",
    "target = 'interpretation'\n",
    "\n",
    "# Codifica as variáveis categóricas e o alvo numérico\n",
    "label_encoders = {}\n",
    "for column in features + [target]:\n",
    "    if df[column].dtype == 'object':\n",
    "        le = LabelEncoder()\n",
    "        df[column] = le.fit_transform(df[column])\n",
    "        label_encoders[column] = le\n",
    "\n",
    "# Divide os dados em conjuntos de treino e teste \n",
    "X = df[features]\n",
    "y = df[target]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Aplica SMOTE para balancear as classes no conjunto de treino\n",
    "smote = SMOTE(random_state=42, k_neighbors=1) # SMOTE (Synthetic Minority Over-sampling Technique) é uma técnica que ajuda a lidar com classes desbalanceadas\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Lista de classificadores que serão testados\n",
    "classifiers = {\n",
    "    # Random Forest: Conjunto de várias árvores de decisão que trabalham em conjunto. Captura relações complexas nos dados, reduzindo overfitting\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced'),\n",
    "\n",
    "    # Logistic Regression: Modelo linear usado para classificação binária ou multiclasse.\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42, class_weight='balanced'),\n",
    "    \n",
    "    # SVM (Support Vector Machine): Classificador que tenta maximizar a margem entre classes. \n",
    "    'SVM': SVC(random_state=42, class_weight='balanced'),\n",
    "    \n",
    "    # Gradient Boosting: Método de aprendizado em conjunto que combina árvores de decisão sequenciais. Cada árvore tenta corrigir erros das anteriores, melhorando a precisão em dados com padrões complexos.\n",
    "    'Gradient Boosting': GradientBoostingClassifier(random_state=42),\n",
    "    \n",
    "    # KNN (K-Nearest Neighbors): Classificador que categoriza novos pontos com base nos \"K\" vizinhos mais próximos\n",
    "    'KNN': KNeighborsClassifier()\n",
    "}\n",
    "\n",
    "# Treina e avalia cada classificador\n",
    "for name, clf in classifiers.items():\n",
    "    print(f\"\\n{name} Results:\")\n",
    "    \n",
    "    # Treina o classificador no conjunto de treino balanceado (SMOTE)\n",
    "    clf.fit(X_train_smote, y_train_smote)\n",
    "    \n",
    "    # Faz previsões no conjunto de teste\n",
    "    y_pred = clf.predict(X_test)\n",
    "    \n",
    "    # Avalia o classificador com a métrica de acurácia\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Accuracy: {accuracy:.2f}\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred, zero_division=1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aprimorando modelo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Random Forest Results:\n",
      "Accuracy: 0.93\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.96      0.96        27\n",
      "           1       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.93        28\n",
      "   macro avg       0.48      0.48      0.48        28\n",
      "weighted avg       0.93      0.93      0.93        28\n",
      "\n",
      "\n",
      "Logistic Regression Results:\n",
      "Accuracy: 0.82\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.85      0.90        27\n",
      "           1       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.82        28\n",
      "   macro avg       0.48      0.43      0.45        28\n",
      "weighted avg       0.92      0.82      0.87        28\n",
      "\n",
      "\n",
      "SVM Results:\n",
      "Accuracy: 0.89\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.89      0.94        27\n",
      "           1       0.25      1.00      0.40         1\n",
      "\n",
      "    accuracy                           0.89        28\n",
      "   macro avg       0.62      0.94      0.67        28\n",
      "weighted avg       0.97      0.89      0.92        28\n",
      "\n",
      "\n",
      "Gradient Boosting Results:\n",
      "Accuracy: 0.89\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.93      0.94        27\n",
      "           1       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.89        28\n",
      "   macro avg       0.48      0.46      0.47        28\n",
      "weighted avg       0.93      0.89      0.91        28\n",
      "\n",
      "\n",
      "KNN Results:\n",
      "Accuracy: 0.86\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.89      0.92        27\n",
      "           1       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.86        28\n",
      "   macro avg       0.48      0.44      0.46        28\n",
      "weighted avg       0.93      0.86      0.89        28\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "# Como tem poucos 1s e muitos 0s, vamos usar o RandomOverSampler para balancear o dataset\n",
    "ros = RandomOverSampler(random_state=42)\n",
    "X_train_resampled, y_train_resampled = ros.fit_resample(X_train, y_train)\n",
    "\n",
    "\n",
    "# Train and evaluate each classifier\n",
    "for name, clf in classifiers.items():\n",
    "    print(f\"\\n{name} Results:\")\n",
    "    \n",
    "    # Train the classifier\n",
    "    clf.fit(X_train_resampled, y_train_resampled)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = clf.predict(X_test)\n",
    "    \n",
    "    # Evaluate the classifier\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Accuracy: {accuracy:.2f}\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred, zero_division=1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Salvando o modelo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvando Pipeline de Dados\n",
    "import pickle   \n",
    "\n",
    "# Salvando modelo em um arquivo model.pkl\n",
    "with open('model.pkl', 'wb') as model_file:\n",
    "    pickle.dump((classifiers, label_encoders), model_file)   \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Páginas HTML\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:5000\n",
      "Press CTRL+C to quit\n",
      "127.0.0.1 - - [26/Oct/2024 22:05:03] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [26/Oct/2024 22:05:03] \"GET /static/estilos.css HTTP/1.1\" 304 -\n",
      "127.0.0.1 - - [26/Oct/2024 22:05:03] \"GET /static/busca.js HTTP/1.1\" 304 -\n",
      "127.0.0.1 - - [26/Oct/2024 22:05:05] \"POST / HTTP/1.1\" 302 -\n",
      "127.0.0.1 - - [26/Oct/2024 22:05:05] \"GET /resposta HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ENTEROBACTERIACEAE': {'R': 0, 'I': 0, 'S': 2}, 'ESCHERICHIA COLI': {'R': 0, 'I': 0, 'S': 6}, 'SERRATIA MARCESCENS': {'R': 0, 'I': 0, 'S': 1}, 'KLEBSIELLA PNEUMONIAE': {'R': 0, 'I': 0, 'S': 1}, 'KLEBSIELLA OXYTOCA': {'R': 0, 'I': 0, 'S': 1}, 'ACINETOBACTER BAUMANNII COMPLEX': {'R': 1, 'I': 2, 'S': 4}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [26/Oct/2024 22:05:05] \"GET /resposta HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [26/Oct/2024 22:05:05] \"GET /static/resposta.css HTTP/1.1\" 304 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ENTEROBACTERIACEAE': {'R': 0, 'I': 0, 'S': 2}, 'ESCHERICHIA COLI': {'R': 0, 'I': 0, 'S': 6}, 'SERRATIA MARCESCENS': {'R': 0, 'I': 0, 'S': 1}, 'KLEBSIELLA PNEUMONIAE': {'R': 0, 'I': 0, 'S': 1}, 'KLEBSIELLA OXYTOCA': {'R': 0, 'I': 0, 'S': 1}, 'ACINETOBACTER BAUMANNII COMPLEX': {'R': 1, 'I': 2, 'S': 4}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [26/Oct/2024 22:05:06] \"GET /static/resposta.js HTTP/1.1\" 304 -\n",
      "C:\\Users\\bitan\\AppData\\Local\\Temp\\ipykernel_23952\\3612384985.py:179: FutureWarning:\n",
      "\n",
      "Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "\n",
      "127.0.0.1 - - [26/Oct/2024 22:05:14] \"POST /predict HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, render_template, request, jsonify, redirect, url_for, session\n",
    "import json\n",
    "import pickle\n",
    "import secrets\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "\n",
    "# Cria uma aplicação Flask e define uma chave secreta para a sessão\n",
    "app = Flask(__name__)\n",
    "app.secret_key = secrets.token_urlsafe(16)\n",
    "\n",
    "# Função usada para encontrar o antibiótico mais próximo do nome pesquisada\n",
    "def levenshtein_distance(s1, s2):\n",
    "    if len(s1) < len(s2):\n",
    "        return levenshtein_distance(s2, s1)\n",
    "\n",
    "    # Inicializa a matriz de distância\n",
    "    previous_row = range(len(s2) + 1)\n",
    "    \n",
    "    for i, c1 in enumerate(s1):\n",
    "        current_row = [i + 1]\n",
    "        for j, c2 in enumerate(s2):\n",
    "            insertions = previous_row[j + 1] + 1\n",
    "            deletions = current_row[j] + 1\n",
    "            substitutions = previous_row[j] + (c1 != c2)\n",
    "            current_row.append(min(insertions, deletions, substitutions))\n",
    "        previous_row = current_row\n",
    "    \n",
    "    return previous_row[-1]\n",
    "\n",
    "# Função para contar resistências de antibióticos por doença com base no medicamento mais próximo\n",
    "def dis_res_count(closest_drug):\n",
    "    # Filtra as doenças baseadas no medicamento mais próximo\n",
    "    diseases = files['microbiology_events'][files['microbiology_events']['ab_name'] == closest_drug]['org_name'].unique()\n",
    "    \n",
    "    # Dicionário para armazenar a contagem de resistência por doença\n",
    "    dis_res_count = {}\n",
    "\n",
    "    # Itera sobre cada doença e conta as interpretações 'R', 'I', 'S'\n",
    "    for disease in diseases:\n",
    "        disease_res = files['microbiology_events'][\n",
    "            (files['microbiology_events']['ab_name'] == closest_drug) & \n",
    "            (files['microbiology_events']['org_name'] == disease)\n",
    "        ]['interpretation']\n",
    "        \n",
    "        # Inicializa a contagem de 'R', 'I', 'S'\n",
    "        counts = {'R': 0, 'I': 0, 'S': 0}\n",
    "        \n",
    "        # Atualiza as contagens com base nas interpretações\n",
    "        counts.update(disease_res.value_counts().to_dict())\n",
    "        \n",
    "        # Armazena a contagem da doença atual\n",
    "        dis_res_count[disease] = counts\n",
    "\n",
    "    return dis_res_count\n",
    "\n",
    "\n",
    "# Função para gerar um gráfico de resistência usando Plotly\n",
    "def plot_resistance_graph(resistencias, closest_drug):\n",
    "    # Prepara dados para o gráfico em formato sunburst\n",
    "    data = {\n",
    "        'character': [],\n",
    "        'parent': [],\n",
    "        'value': [],\n",
    "        'ids': [],\n",
    "        'labels': [],\n",
    "    }\n",
    "\n",
    "    # Adiciona o antibiótico como raiz do gráfico\n",
    "    data['character'].append(closest_drug)\n",
    "    data['ids'].append(closest_drug)\n",
    "    data['labels'].append(closest_drug)\n",
    "    data['parent'].append('')\n",
    "    data['value'].append(sum([sum(counts.values())\n",
    "                         for counts in resistencias.values()]))\n",
    "\n",
    "    # Preenche dados para cada bactéria e contagens de resistência\n",
    "    for bacteria, counts in resistencias.items():\n",
    "        # Adiciona bactérias a partir dos antibióticos\n",
    "        data['ids'].append(bacteria)\n",
    "        data['labels'].append(bacteria)\n",
    "        data['character'].append(bacteria)\n",
    "        data['parent'].append(closest_drug)\n",
    "        data['value'].append(sum(counts.values()))\n",
    "\n",
    "        # Adiciona níveis de resistência a partir de cada bactéria\n",
    "        for i, (interpretation, count) in enumerate(counts.items()):\n",
    "            if count > 0:\n",
    "                unique_interpretation = f\"{interpretation}_{bacteria}\"\n",
    "                data['ids'].append(unique_interpretation)\n",
    "                data['labels'].append(interpretation)\n",
    "                data['character'].append(unique_interpretation)\n",
    "                data['parent'].append(bacteria)\n",
    "                data['value'].append(count)\n",
    "\n",
    "   # Cria o gráfico sunburst\n",
    "    fig = px.sunburst(\n",
    "        data,\n",
    "        ids='ids',          # Unique IDs\n",
    "        names='labels',      # Display names\n",
    "        parents='parent',   # Parent-child relationships using IDs\n",
    "        values='value',     # Values for each level\n",
    "        color='labels',      # Color based on label names\n",
    "        color_discrete_map={'R': 'red', 'I': 'yellow', 'S': 'green'},\n",
    "        height=800,\n",
    "        width=800\n",
    "    )\n",
    "\n",
    "    # Atualiza o layout para transparência e tamanho\n",
    "    fig.update_layout(\n",
    "        template=None,\n",
    "        paper_bgcolor='rgba(0,0,0,0)',\n",
    "        plot_bgcolor='rgba(0,0,0,0)',\n",
    "    )\n",
    "\n",
    "    # Converte o gráfico para HTML\n",
    "    graph_html = pio.to_html(fig, full_html=False)\n",
    "    return graph_html\n",
    "\n",
    "\n",
    "# Rota principal (página inicial) com busca de antibiótico\n",
    "@app.route('/', methods=['GET', 'POST'])\n",
    "def home():\n",
    "    if request.method == 'GET':\n",
    "        return render_template('busca.html')\n",
    "    elif request.method == 'POST':\n",
    "        ab_name = request.form.get('ab_name') # Recebe o nome do antibiótico do formulário\n",
    "\n",
    "        def search(term):\n",
    "            all_drugs = files['microbiology_events']['ab_name'].dropna().unique()\n",
    "\n",
    "            closest_drug = None    \n",
    "            for drug in all_drugs:\n",
    "                dist = levenshtein_distance(term, drug)\n",
    "                if closest_drug is None or dist < closest_drug[1]:\n",
    "                   closest_drug = (drug, dist)\n",
    "            return closest_drug\n",
    "        \n",
    "\n",
    "        if ab_name:\n",
    "            droga = search(ab_name)\n",
    "            session['droga'] = droga[0]  # Armazena apenas o nome da droga na sessão\n",
    "            return redirect(url_for('resposta'))\n",
    "        else:\n",
    "            return jsonify({\"error\": \"Nome do antibiótico não informado\"}), 400\n",
    "\n",
    "# Rota para a página secundaria (resposta)\n",
    "\n",
    "@app.route('/resposta')\n",
    "def resposta():\n",
    "    closest_drug = session.get('droga')  # Recupera a droga da sessão\n",
    "\n",
    "    # Verifica se o medicamento foi encontrado\n",
    "    if closest_drug:\n",
    "        # Chama a função que calcula as contagens de resistência\n",
    "        resistencias = dis_res_count(closest_drug)\n",
    "        graph_html = plot_resistance_graph(resistencias, closest_drug) # Rota para a página de resposta que exibe a resistência\n",
    "        print(resistencias)\n",
    "    else:\n",
    "        resistencias = {}\n",
    "        graph_html = None\n",
    "\n",
    "    # Renderiza o template com os dados de resistência\n",
    "    return render_template('resposta.html', closest_drug=closest_drug, resistencias=resistencias, graph_html=graph_html)\n",
    "\n",
    "\n",
    "# Função para predição de resistência via AJAX\n",
    "@app.route('/predict', methods=['POST'])\n",
    "def run_prediction():\n",
    "    if request.method == \"POST\":\n",
    "        byte_string = request.data.decode('utf-8')\n",
    "        data_dict = {item['name']: item['value'] for item in json.loads(byte_string)}\n",
    "        \n",
    "        # Transforma o dicionário em DataFrame para processar\n",
    "        data = pd.DataFrame(data_dict, index=[0])\n",
    "        selected_columns = [\"Cardiovascular\", \"Respiratory\", \"Neurological\", \"Infectious\", \n",
    "                        \"Gastrointestinal\", \"Renal\", \"Endocrine\", \"Oncology\", \"Trauma\", \"Other\"]\n",
    "        \n",
    "        data[selected_columns] = data[selected_columns].replace({'Low': 0, 'Medium': 1, 'High': 2})\n",
    "\n",
    "        # Define valores únicos para colunas categóricas\n",
    "        all_ins = files['admissions']['insurance'].dropna().unique()\n",
    "        all_rels = files['admissions']['religion'].dropna().unique()\n",
    "        all_mar = files['admissions']['marital_status'].dropna().unique()\n",
    "        all_eth = files['admissions']['ethnicity'].dropna().unique()\n",
    "        all_gend = files['patients']['gender'].dropna().unique()\n",
    "        all_drugs = files['microbiology_events']['ab_name'].dropna().unique()\n",
    "\n",
    "\n",
    "        col_to_all = {\n",
    "            'insurance': all_ins,\n",
    "            'religion': all_rels,\n",
    "            'marital_status': all_mar,\n",
    "            'ethnicity': all_eth,\n",
    "            'gender': all_gend,\n",
    "            'ab_name': all_drugs\n",
    "        }\n",
    "\n",
    "        # Define as colunas categóricas do conjunto de dados que serão comparadas para encontrar correspondências próximas\n",
    "        text_columns = ['insurance', 'religion', 'marital_status', 'ethnicity', 'gender', 'ab_name']\n",
    "\n",
    "\n",
    "        # Dicionário para armazenar as correspondências mais próximas para cada coluna\n",
    "        closest_received_data = {}\n",
    "\n",
    "        # Itera sobre o conjunto de dados recebido, encontrando a menor distância entre cada valor recebido e os valores do conjunto de dados de treino\n",
    "        for text_col in text_columns:\n",
    "            closest_match = None\n",
    "            closest_distance = float('inf')  # Inicia com um valor alto para comparação inicial\n",
    "            for received_col in data[text_col]:  # Itera sobre os valores recebidos para a coluna atual\n",
    "                for all_col in col_to_all[text_col]:  # Itera sobre os valores do conjunto de dados treinado para a coluna atual\n",
    "                    dist = levenshtein_distance(received_col, all_col)\n",
    "\n",
    "                    # Atualiza o valor mais próximo se uma correspondência mais próxima for encontrada\n",
    "                    if dist < closest_distance:\n",
    "                        closest_match = all_col\n",
    "                        closest_distance = dist\n",
    "\n",
    "            # Armazena a correspondência mais próxima para a coluna atual\n",
    "            closest_received_data[text_col] = closest_match\n",
    "        # Converte as correspondências mais próximas para um DataFrame\n",
    "        text_df = pd.DataFrame(closest_received_data, index=[0])\n",
    "        # Concat as colunas categóricas ajustadas com os dados restantes selecionados para a previsão\n",
    "        data_to_predict = pd.concat([data[selected_columns], text_df], axis=1).iloc[0:1]\n",
    "        # Define a ordem das colunas no conjunto de dados de previsão, alinhando-as com o modelo de machine learning\n",
    "        data_to_predict = data_to_predict[[\n",
    "            'insurance', 'religion', 'marital_status', 'ethnicity', 'gender', 'ab_name',\n",
    "            'Cardiovascular', 'Respiratory', 'Neurological', 'Infectious', \n",
    "            'Gastrointestinal', 'Renal', 'Endocrine', 'Oncology', 'Trauma', 'Other'\n",
    "        ]].iloc[0:1]\n",
    "    \n",
    "\n",
    "        # Carrega os modelos e os codificadores de rótulos\n",
    "        models, label_encoders = pickle.load(open('model.pkl', 'rb'))\n",
    "\n",
    "        # Transforma os valores categóricos para o formato esperado pelo modelo, utilizando os codificadores de rótulos\n",
    "        for column, encoder in label_encoders.items():\n",
    "            data_to_predict[column] = encoder.transform(data_to_predict[column])\n",
    "\n",
    "        # Inicializa a lista de previsões\n",
    "        predictions = []\n",
    "        for name, classifier in models.items():\n",
    "            prediction = classifier.predict(data_to_predict)\n",
    "            predictions.append(prediction)\n",
    "\n",
    "        # Calcula a pontuação percentual de previsões positivas (resistência encontrada)\n",
    "        score = sum(pred == 1 for pred in predictions)\n",
    "        percent_score = (score / len(predictions)) * 100\n",
    "\n",
    "        # Retorna a pontuação percentual em formato JSON\n",
    "        return jsonify({'score': percent_score[0]}), 200\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run() \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
