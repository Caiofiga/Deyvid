{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INGESTÃO DE DADOS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "file_paths = {\n",
    "    'admissions': 'rawcsvs/ADMISSIONS.csv',\n",
    "    'microbiology_events': 'rawcsvs/MICROBIOLOGYEVENTS.csv',\n",
    "    'prescriptions': 'rawcsvs/PRESCRIPTIONS.csv',\n",
    "    'labevents': 'rawcsvs/LABEVENTS.csv',\n",
    "    'diagnoses_icd': 'rawcsvs/DIAGNOSES_ICD.csv',\n",
    "    'patients': 'rawcsvs/PATIENTS.csv',\n",
    "    'transl-labitems': 'rawcsvs/D_LABITEMS.csv',\n",
    "    'transl-diag': 'rawcsvs/D_ICD_DIAGNOSES.csv'\n",
    "}\n",
    "\n",
    "cols_to_use = {\n",
    "    'admissions': ['subject_id','admittime','ethnicity', 'admission_type'],\n",
    "    'microbiology_events': ['row_id', 'subject_id',  'chartdate', 'charttime', 'spec_itemid', 'spec_type_desc', \n",
    "                            'org_itemid', 'org_name', 'isolate_num', 'ab_itemid', 'ab_name', 'dilution_text', \n",
    "                            'dilution_comparison', 'dilution_value', 'interpretation'],\n",
    "    'prescriptions': ['subject_id', \"hadm_id\", 'drug', 'enddate'],\n",
    "    'labevents': ['subject_id', 'itemid', 'valuenum', 'valueuom', 'flag'],\n",
    "    'diagnoses_icd': ['subject_id', 'icd9_code'], #gotta work on this one, adding all codes to one line in the array\n",
    "    'transl-labitems': ['itemid', 'label'], # use this in conjunction with labevents to understando wtf if happening\n",
    "    'patients': ['subject_id', 'gender'],\n",
    "    'transl-diag': ['icd9_code', 'short_title', 'long_title']\n",
    "}\n",
    "\n",
    "files = {}\n",
    "for file_path in file_paths:\n",
    "    files[file_path] = pd.read_csv(file_paths[file_path], usecols=cols_to_use[file_path])\n",
    "    \n",
    "for file_path, file in files.items(): \n",
    "    if  not file_path.startswith('transl'):\n",
    "        file.dropna(inplace=True, subset=['subject_id'])\n",
    "        file.drop_duplicates(inplace=True)  \n",
    "        file_name = os.path.basename(file_path)\n",
    "        file.to_csv(f'cleanedcsv/{file_name}.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MANEIRA PARA JUNTAR TODOS OS DADOS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Isso esta feito para essas duas tabelas\n",
    "o modo nao mudou, precisa mudar as tabelas importadas, as colunas pivoteadas, e o valor de value, dependendo do que \n",
    "precisa ser pivoteado\n",
    "Agradeco Pedro por me dar a idea de pivotear as tabelas\n",
    "'''\n",
    "import pandas as pd\n",
    "mergedata = pd.read_csv('mergedcsv/admin-diag-lab-micro.csv')\n",
    "diagnoses = pd.read_csv('cleanedcsv/prescriptions.csv')\n",
    "\n",
    "\n",
    "#inverter a tabela de diagnósticos para ter uma linha por paciente\n",
    "\n",
    "diaginv = diagnoses.assign(value=1).pivot_table(\n",
    "    index='subject_id', \n",
    "    columns='drug', \n",
    "    values='value', \n",
    "    fill_value=0\n",
    ").reset_index()    # Reset the index to make `subject_id` a column\n",
    "\n",
    "diaginv.fillna(0, inplace=True)\n",
    "\n",
    "# Now proceed with the merge\n",
    "mergedata = mergedata.merge(diaginv, on='subject_id', how='left')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TRADUCAO DE CODIGOS PARA TEXTOS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "#Need to convert the icd9_codes into text\n",
    "def tryconvertint(possibleint):\n",
    "    try:\n",
    "        x = int(possibleint)\n",
    "        return x\n",
    "    except ValueError:\n",
    "        return possibleint \n",
    "\n",
    "#first, I will get a representation of all the possible codes and their respective values\n",
    "unique_diag_codes = pd.read_csv(os.path.join('rawcsvs', 'D_ICD_DIAGNOSES.csv')).filter(['icd9_code', 'short_title']).drop_duplicates()\n",
    "unique_labitem_codes = pd.read_csv(os.path.join('rawcsvs', 'D_LABITEMS.csv')).filter(['itemid', 'label']).drop_duplicates()\n",
    "\n",
    "#second, I will iterate through each column name and replace it with the title and/or label\n",
    "allthedata = pd.read_csv(os.path.join('mergedcsv', 'admin-diag-lab-micro-pat-prescrip.csv'))\n",
    "\n",
    "new_columns = []\n",
    "\n",
    "for column in allthedata.columns:\n",
    "    colint = tryconvertint(column)\n",
    "    if colint == 50800:\n",
    "        x = unique_labitem_codes['itemid'].values\n",
    "        pass\n",
    "\n",
    "    # Check if the column is in the 'icd9_code' of unique_diag_codes\n",
    "    if column in unique_diag_codes['icd9_code'].values:\n",
    "        # Map it to 'short_title'\n",
    "        new_columns.append(unique_diag_codes.loc[unique_diag_codes['icd9_code'] == column, 'short_title'].values[0])\n",
    "\n",
    "    # Check if the column is in the 'itemid' of unique_labitem_codes\n",
    "    elif colint in unique_labitem_codes['itemid'].values:\n",
    "        # Map it to 'label'\n",
    "        new_columns.append(unique_labitem_codes.loc[unique_labitem_codes['itemid'] == colint, 'label'].values[0])\n",
    "\n",
    "    # If not found, retain the original column name\n",
    "    else:\n",
    "        new_columns.append(column)\n",
    "\n",
    "allthedata.columns = new_columns\n",
    "allthedata.columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EXTRACAO DE DADOS IMPORTANTES A BUSCA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Levenshtein\n",
    "#supunhetemos que tenhamos uma busca\n",
    "ab_name = 'vancomycin'  # Define a default value for ab_name\n",
    "search = ab_name if ab_name else 'vancomycin'\n",
    "#precisamos listar todas as doencas em que o medicamento foi usado\n",
    "#para facilitar a busca, precisamos calcular o medicamento mais provavel que foi escrito\n",
    "all_drugs = files['microbiology_events']['ab_name'].dropna().unique()\n",
    "\n",
    "closest_drug = None\t\n",
    "for drug in all_drugs:\n",
    "    dist = Levenshtein.distance(search, drug)\n",
    "    if closest_drug is None or dist < closest_drug[1]:\n",
    "        closest_drug = (drug, dist)\n",
    "\n",
    "#agora que temos o medicamento mais provavel, podemos listar as resistencias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coletando os dados de resistencia\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "#need to sort he diseases\n",
    "closest_drug = files['microbiology_events']['ab_name'].mode()[0]\n",
    "diseases = files['microbiology_events'][files['microbiology_events']['ab_name'] == closest_drug[0]]['org_name'].unique()\n",
    "\n",
    "\n",
    "\n",
    "dis_res_count = {}\n",
    "for disease in diseases:\n",
    "    disease_res = files['microbiology_events'][\n",
    "        (files['microbiology_events']['ab_name'] == closest_drug[0]) & \n",
    "        (files['microbiology_events']['org_name'] == disease)\n",
    "    ]['interpretation']\n",
    "    \n",
    "    # Initialize counts for 'R', 'I', 'S'\n",
    "    counts = {'R': 0, 'I': 0, 'S': 0}\n",
    "    \n",
    "    counts.update(disease_res.value_counts().to_dict())\n",
    "    \n",
    "    dis_res_count[disease] = counts\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FODASSE O MODELO PREDITIVO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, render_template, request, jsonify, redirect, url_for, session\n",
    "import Levenshtein\n",
    "\n",
    "app = Flask(__name__)\n",
    "app.secret_key = 'VAI SE FUDER FIGA' #perdao bita cabrita\n",
    "\n",
    "@app.route('/', methods=['GET', 'POST'])\n",
    "def home():\n",
    "    if request.method == 'GET':\n",
    "        return render_template('busca.html')\n",
    "    elif request.method == 'POST':\n",
    "        ab_name = request.form.get('ab_name')\n",
    "\n",
    "        def search(term):\n",
    "            all_drugs = files['microbiology_events']['ab_name'].dropna().unique()\n",
    "\n",
    "            closest_drug = None    \n",
    "            for drug in all_drugs:\n",
    "                dist = Levenshtein.distance(term, drug)\n",
    "                if closest_drug is None or dist < closest_drug[1]:\n",
    "                    closest_drug = (drug, dist)\n",
    "            return closest_drug\n",
    "        \n",
    "\n",
    "        if ab_name:\n",
    "            droga = search(ab_name)\n",
    "            session['droga'] = droga[0]  # Armazena apenas o nome da droga na sessão\n",
    "            return redirect(url_for('resposta'))\n",
    "        else:\n",
    "            return jsonify({\"error\": \"Nome do antibiótico não informado\"}), 400\n",
    "\n",
    "# Rota para a página secundaria (resposta)\n",
    "\n",
    "@app.route('/resposta')\n",
    "\n",
    "def resposta():\n",
    "    closest_drug = session.get('droga')  # Recupera a droga da sessão\n",
    "\n",
    "    # Verifica se o medicamento foi encontrado\n",
    "    if closest_drug:\n",
    "        # Chama a função que calcula as contagens de resistência\n",
    "        resistencias = dis_res_count(closest_drug)\n",
    "    else:\n",
    "        resistencias = {}\n",
    "\n",
    "    # Renderiza o template com os dados de resistência\n",
    "    return render_template('resposta.html', closest_drug=closest_drug, resistencias=resistencias)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run() \n",
    "\n",
    "\n",
    "def dis_res_count(closest_drug):\n",
    "    # Filtra as doenças baseadas no medicamento mais próximo\n",
    "    diseases = files['microbiology_events'][files['microbiology_events']['ab_name'] == closest_drug]['org_name'].unique()\n",
    "    \n",
    "    # Dicionário para armazenar a contagem de resistência por doença\n",
    "    dis_res_count = {}\n",
    "\n",
    "    # Itera sobre cada doença e conta as interpretações 'R', 'I', 'S'\n",
    "    for disease in diseases:\n",
    "        disease_res = files['microbiology_events'][\n",
    "            (files['microbiology_events']['ab_name'] == closest_drug) & \n",
    "            (files['microbiology_events']['org_name'] == disease)\n",
    "        ]['interpretation']\n",
    "        \n",
    "        # Inicializa a contagem de 'R', 'I', 'S'\n",
    "        counts = {'R': 0, 'I': 0, 'S': 0}\n",
    "        \n",
    "        # Atualiza as contagens com base nas interpretações\n",
    "        counts.update(disease_res.value_counts().to_dict())\n",
    "        \n",
    "        # Armazena a contagem da doença atual\n",
    "        dis_res_count[disease] = counts\n",
    "\n",
    "    return dis_res_count\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Criando o dataset para o Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os \n",
    "\n",
    "patients = pd.read_csv(os.path.join('rawcsvs', 'PATIENTS.csv'))\n",
    "admissions = pd.read_csv(os.path.join('rawcsvs', 'ADMISSIONS.csv'))\n",
    "microbiology = pd.read_csv(os.path.join('rawcsvs', 'MICROBIOLOGYEVENTS.csv'))\n",
    "\n",
    "\n",
    "# Merge the patients and admissions dataframes  \n",
    "new_data = (admissions\n",
    "            .filter(['subject_id', 'admission_type', 'ethnicity', 'diagnosis', 'hospital_expire_flag', 'insurance', 'religion', 'marital_status'])\n",
    "            #.drop(admissions[admissions['hospital_expire_flag'] == 1].index)\n",
    "            .merge(patients, on='subject_id', how='left')\n",
    "            )\n",
    "#merge the microbiology data\n",
    "new_data = (\n",
    "    microbiology\n",
    "    .filter(['subject_id', 'ab_name', 'interpretation'])\n",
    "    # One hot encoding the interpretation\n",
    "    .assign(value=(microbiology['interpretation'] == 'R').astype(int))\n",
    "    .reset_index(drop=True)\n",
    "    .merge(new_data, on='subject_id', how='left')\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "aux1 = ( # one hot enconding e passando para formato largo os diagnósticos\n",
    "    new_data\n",
    "    .filter(['subject_id', 'diagnosis'])\n",
    "    .assign(value=1)\n",
    "    .pivot_table(index='subject_id', columns='diagnosis', values='value', fill_value=0)\n",
    "    .reset_index()\n",
    ")\n",
    "aux2 = (\n",
    "    new_data\n",
    "    .groupby('subject_id')\n",
    "    .agg({\n",
    "        'insurance': 'first',\n",
    "        'religion': 'first',\n",
    "        'marital_status': 'first',\n",
    "        'ethnicity': 'first',\n",
    "        'gender': 'first',\n",
    "        'ab_name': 'first',\n",
    "        'interpretation': 'first'\n",
    "    })\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "filtered_data = aux1.merge(aux2, on='subject_id', how='left')\n",
    "filtered_data.to_csv('improving.csv', index=False)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Engineering (Fuck data science)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for +: 'float' and 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[69], line 54\u001b[0m\n\u001b[1;32m     51\u001b[0m cols \u001b[38;5;241m=\u001b[39m [col \u001b[38;5;28;01mfor\u001b[39;00m col, types \u001b[38;5;129;01min\u001b[39;00m disease_to_type\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m disease_type \u001b[38;5;129;01min\u001b[39;00m types]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cols:\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;66;03m# Sum the columns (since they are binary, this counts the number of diseases)\u001b[39;00m\n\u001b[0;32m---> 54\u001b[0m     df[disease_type] \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcols\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     56\u001b[0m     df[disease_type] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m  \u001b[38;5;66;03m# If no diseases of this type are present\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/275c0912-f51b-4184-aecd-73c872548943/VScode/Projetos/Deyvid/.venv/lib/python3.12/site-packages/pandas/core/frame.py:11670\u001b[0m, in \u001b[0;36mDataFrame.sum\u001b[0;34m(self, axis, skipna, numeric_only, min_count, **kwargs)\u001b[0m\n\u001b[1;32m  11661\u001b[0m \u001b[38;5;129m@doc\u001b[39m(make_doc(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msum\u001b[39m\u001b[38;5;124m\"\u001b[39m, ndim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m))\n\u001b[1;32m  11662\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msum\u001b[39m(\n\u001b[1;32m  11663\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m  11668\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m  11669\u001b[0m ):\n\u001b[0;32m> 11670\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskipna\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumeric_only\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_count\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m  11671\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msum\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/mnt/275c0912-f51b-4184-aecd-73c872548943/VScode/Projetos/Deyvid/.venv/lib/python3.12/site-packages/pandas/core/generic.py:12506\u001b[0m, in \u001b[0;36mNDFrame.sum\u001b[0;34m(self, axis, skipna, numeric_only, min_count, **kwargs)\u001b[0m\n\u001b[1;32m  12498\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msum\u001b[39m(\n\u001b[1;32m  12499\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m  12500\u001b[0m     axis: Axis \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m  12504\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m  12505\u001b[0m ):\n\u001b[0;32m> 12506\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_min_count_stat_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m  12507\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msum\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnanops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnansum\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskipna\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumeric_only\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_count\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m  12508\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/275c0912-f51b-4184-aecd-73c872548943/VScode/Projetos/Deyvid/.venv/lib/python3.12/site-packages/pandas/core/generic.py:12489\u001b[0m, in \u001b[0;36mNDFrame._min_count_stat_function\u001b[0;34m(self, name, func, axis, skipna, numeric_only, min_count, **kwargs)\u001b[0m\n\u001b[1;32m  12486\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m axis \u001b[38;5;129;01mis\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mno_default:\n\u001b[1;32m  12487\u001b[0m     axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m> 12489\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reduce\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m  12490\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m  12491\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m  12492\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m  12493\u001b[0m \u001b[43m    \u001b[49m\u001b[43mskipna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskipna\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m  12494\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnumeric_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnumeric_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m  12495\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmin_count\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmin_count\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m  12496\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/275c0912-f51b-4184-aecd-73c872548943/VScode/Projetos/Deyvid/.venv/lib/python3.12/site-packages/pandas/core/frame.py:11562\u001b[0m, in \u001b[0;36mDataFrame._reduce\u001b[0;34m(self, op, name, axis, skipna, numeric_only, filter_type, **kwds)\u001b[0m\n\u001b[1;32m  11558\u001b[0m     df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mT\n\u001b[1;32m  11560\u001b[0m \u001b[38;5;66;03m# After possibly _get_data and transposing, we are now in the\u001b[39;00m\n\u001b[1;32m  11561\u001b[0m \u001b[38;5;66;03m#  simple case where we can use BlockManager.reduce\u001b[39;00m\n\u001b[0;32m> 11562\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblk_func\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m  11563\u001b[0m out \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39m_constructor_from_mgr(res, axes\u001b[38;5;241m=\u001b[39mres\u001b[38;5;241m.\u001b[39maxes)\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m  11564\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m out_dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m out\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mboolean\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m/mnt/275c0912-f51b-4184-aecd-73c872548943/VScode/Projetos/Deyvid/.venv/lib/python3.12/site-packages/pandas/core/internals/managers.py:1500\u001b[0m, in \u001b[0;36mBlockManager.reduce\u001b[0;34m(self, func)\u001b[0m\n\u001b[1;32m   1498\u001b[0m res_blocks: \u001b[38;5;28mlist\u001b[39m[Block] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m   1499\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m blk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks:\n\u001b[0;32m-> 1500\u001b[0m     nbs \u001b[38;5;241m=\u001b[39m \u001b[43mblk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1501\u001b[0m     res_blocks\u001b[38;5;241m.\u001b[39mextend(nbs)\n\u001b[1;32m   1503\u001b[0m index \u001b[38;5;241m=\u001b[39m Index([\u001b[38;5;28;01mNone\u001b[39;00m])  \u001b[38;5;66;03m# placeholder\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/275c0912-f51b-4184-aecd-73c872548943/VScode/Projetos/Deyvid/.venv/lib/python3.12/site-packages/pandas/core/internals/blocks.py:404\u001b[0m, in \u001b[0;36mBlock.reduce\u001b[0;34m(self, func)\u001b[0m\n\u001b[1;32m    398\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreduce\u001b[39m(\u001b[38;5;28mself\u001b[39m, func) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[Block]:\n\u001b[1;32m    400\u001b[0m     \u001b[38;5;66;03m# We will apply the function and reshape the result into a single-row\u001b[39;00m\n\u001b[1;32m    401\u001b[0m     \u001b[38;5;66;03m#  Block with the same mgr_locs; squeezing will be done at a higher level\u001b[39;00m\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m--> 404\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    406\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    407\u001b[0m         res_values \u001b[38;5;241m=\u001b[39m result\n",
      "File \u001b[0;32m/mnt/275c0912-f51b-4184-aecd-73c872548943/VScode/Projetos/Deyvid/.venv/lib/python3.12/site-packages/pandas/core/frame.py:11481\u001b[0m, in \u001b[0;36mDataFrame._reduce.<locals>.blk_func\u001b[0;34m(values, axis)\u001b[0m\n\u001b[1;32m  11479\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray([result])\n\u001b[1;32m  11480\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m> 11481\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskipna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskipna\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/275c0912-f51b-4184-aecd-73c872548943/VScode/Projetos/Deyvid/.venv/lib/python3.12/site-packages/pandas/core/nanops.py:85\u001b[0m, in \u001b[0;36mdisallow.__call__.<locals>._f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m     82\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreduction operation \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m not allowed for this dtype\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     83\u001b[0m     )\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 85\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;66;03m# we want to transform an object array\u001b[39;00m\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;66;03m# ValueError message to the more typical TypeError\u001b[39;00m\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;66;03m# e.g. this is normally a disallowed function on\u001b[39;00m\n\u001b[1;32m     90\u001b[0m     \u001b[38;5;66;03m# object arrays that contain strings\u001b[39;00m\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_object_dtype(args[\u001b[38;5;241m0\u001b[39m]):\n",
      "File \u001b[0;32m/mnt/275c0912-f51b-4184-aecd-73c872548943/VScode/Projetos/Deyvid/.venv/lib/python3.12/site-packages/pandas/core/nanops.py:404\u001b[0m, in \u001b[0;36m_datetimelike_compat.<locals>.new_func\u001b[0;34m(values, axis, skipna, mask, **kwargs)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m datetimelike \u001b[38;5;129;01mand\u001b[39;00m mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    402\u001b[0m     mask \u001b[38;5;241m=\u001b[39m isna(values)\n\u001b[0;32m--> 404\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskipna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskipna\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m datetimelike:\n\u001b[1;32m    407\u001b[0m     result \u001b[38;5;241m=\u001b[39m _wrap_results(result, orig_values\u001b[38;5;241m.\u001b[39mdtype, fill_value\u001b[38;5;241m=\u001b[39miNaT)\n",
      "File \u001b[0;32m/mnt/275c0912-f51b-4184-aecd-73c872548943/VScode/Projetos/Deyvid/.venv/lib/python3.12/site-packages/pandas/core/nanops.py:477\u001b[0m, in \u001b[0;36mmaybe_operate_rowwise.<locals>.newfunc\u001b[0;34m(values, axis, **kwargs)\u001b[0m\n\u001b[1;32m    474\u001b[0m         results \u001b[38;5;241m=\u001b[39m [func(x, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m arrs]\n\u001b[1;32m    475\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(results)\n\u001b[0;32m--> 477\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/275c0912-f51b-4184-aecd-73c872548943/VScode/Projetos/Deyvid/.venv/lib/python3.12/site-packages/pandas/core/nanops.py:646\u001b[0m, in \u001b[0;36mnansum\u001b[0;34m(values, axis, skipna, min_count, mask)\u001b[0m\n\u001b[1;32m    643\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m dtype\u001b[38;5;241m.\u001b[39mkind \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mm\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    644\u001b[0m     dtype_sum \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdtype(np\u001b[38;5;241m.\u001b[39mfloat64)\n\u001b[0;32m--> 646\u001b[0m the_sum \u001b[38;5;241m=\u001b[39m \u001b[43mvalues\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype_sum\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    647\u001b[0m the_sum \u001b[38;5;241m=\u001b[39m _maybe_null_out(the_sum, axis, mask, values\u001b[38;5;241m.\u001b[39mshape, min_count\u001b[38;5;241m=\u001b[39mmin_count)\n\u001b[1;32m    649\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m the_sum\n",
      "File \u001b[0;32m/mnt/275c0912-f51b-4184-aecd-73c872548943/VScode/Projetos/Deyvid/.venv/lib/python3.12/site-packages/numpy/_core/_methods.py:53\u001b[0m, in \u001b[0;36m_sum\u001b[0;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_sum\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     52\u001b[0m          initial\u001b[38;5;241m=\u001b[39m_NoValue, where\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m---> 53\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mumr_sum\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for +: 'float' and 'str'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Assuming your DataFrame is called 'df'\n",
    "df = pd.read_csv('improving.csv', sep=',')\n",
    "\n",
    "# Step 1: Identify non-disease columns\n",
    "non_disease_columns = ['subject_id', 'insurance', 'religion', 'marital_status', 'ethnicity', 'gender', 'ab_name', 'interpretation'] \n",
    "\n",
    "# Identify disease columns by excluding non-disease columns\n",
    "disease_columns = [col for col in df.columns if col not in non_disease_columns]\n",
    "\n",
    "# Step 2: Create a mapping from each disease to disease types\n",
    "# Define your disease types and map diseases to them\n",
    "disease_type_mapping = {\n",
    "    'Cardiovascular': ['ACUTE PULMONARY EMBOLISM', 'CONGESTIVE HEART FAILURE', 'CORONARY ARTERY DISEASE', 'STEMI', 'MI CHF', 'BRADYCARDIA', 'VF ARREST', 'PERICARDIAL EFFUSION', 'PULMONARY EDEMA, MI', 'PULMONARY EDEMA\\\\CATH', 'CRITICAL AORTIC STENOSIS/HYPOTENSION', 'INFERIOR MYOCARDIAL INFARCTION\\\\CATH'],\n",
    "    'Respiratory': ['ACUTE RESPIRATORY DISTRESS SYNDROME', 'ASTHMA', 'COPD FLARE', 'PNEUMONIA', 'RESPIRATORY DISTRESS', 'SHORTNESS OF BREATH', 'HYPOXIA', 'TACHYPNEA', 'PLEURAL EFFUSION', 'MEDIASTINAL ADENOPATHY'],\n",
    "    'Neurological': ['ALTERED MENTAL STATUS', 'SEIZURE', 'STATUS EPILEPTICUS', 'STROKE/TIA', 'INTRACRANIAL HEMORRHAGE', 'HEADACHE', 'FACIAL NUMBNESS', 'BASAL GANGLIN BLEED', 'ALTERED MENTAL STATUS'],\n",
    "    'Infectious': ['CELLULITIS', 'FEVER', 'SEPSIS', 'URINARY TRACT INFECTION', 'UROSEPSIS', 'PYELONEPHRITIS', 'UTI', 'HIV', 'ABSCESS', 'PNEUMONIA;TELEMETRY', 'SEPSIS; UTI', 'FEVER;URINARY TRACT INFECTION'],\n",
    "    'Gastrointestinal': ['ABDOMINAL PAIN', 'GASTROINTESTINAL BLEED', 'LOWER GI BLEED', 'UPPER GI BLEED', 'VARICEAL BLEED', 'HEPATITIS', 'LIVER FAILURE', 'CHOLECYSTITIS', 'CHOLANGITIS', 'PANCREATITIS', 'ACUTE CHOLECYSTITIS', 'ACUTE CHOLANGITIS', 'HEPATITIS B', 'ALCOHOLIC HEPATITIS', 'ELEVATED LIVER FUNCTIONS;S/P LIVER TRANSPLANT', 'FAILURE TO THRIVE'],\n",
    "    'Renal': ['ACUTE RENAL FAILURE', 'CHRONIC KIDNEY DISEASE', 'RENAL CANCER', 'HYPOTENSION, RENAL FAILURE'],\n",
    "    'Endocrine': ['DIABETES', 'HYPOGLYCEMIA', 'HYPERGLYCEMIA', 'HYPONATREMIA'],\n",
    "    'Oncology': ['LUNG CANCER', 'BREAST CANCER', 'COLON CANCER', 'LEUKEMIA', 'LYMPHOMA', 'METASTATIC MELANOMA', 'NON SMALL CELL CANCER', 'METASTIC MELANOMA;ANEMIA', 'CHRONIC MYELOGENOUS LEUKEMIA;TRANSFUSION REACTION'],\n",
    "    'Trauma': ['FRACTURE', 'MOTOR VEHICLE ACCIDENT', 'SUBDURAL HEMATOMA', 'LEFT HIP FRACTURE', 'RIGHT HUMEROUS FRACTURE', 'HUMERAL FRACTURE', 'S/P MOTOR VEHICLE ACCIDENT', 'ACUTE SUBDURAL HEMATOMA'],\n",
    "    'Other': ['OVERDOSE', 'FAILURE TO THRIVE', 'MEDIASTINAL ADENOPATHY', 'ARREST', 'OVERDOSE', 'AROMEGLEY;BURKITTS LYMPHOMA', 'TRACHEAL ESOPHAGEAL FISTULA', 'TRACHEAL STENOSIS', 'VOLVULUS', 'ANEMIA', 'BURKITTS LYMPHOMA', 'NON SMALL CELL CANCER;HYPOXIA', 'METASTATIC MELANOMA;BRAIN METASTASIS']\n",
    "}\n",
    "\n",
    "# Initialize a dictionary to map each disease column to its disease type(s)\n",
    "disease_to_type = {}\n",
    "\n",
    "# Process each disease column\n",
    "for col in disease_columns:\n",
    "    # Split disease names if they contain multiple diseases\n",
    "    diseases = re.split(';|,|/', col)\n",
    "    diseases = [disease.strip().upper() for disease in diseases]\n",
    "    types = set()\n",
    "    for disease in diseases:\n",
    "        # Find the disease type for this disease\n",
    "        found = False\n",
    "        for disease_type, disease_list in disease_type_mapping.items():\n",
    "            if disease in disease_list:\n",
    "                types.add(disease_type)\n",
    "                found = True\n",
    "        if not found:\n",
    "            types.add('Other')  # Assign 'Other' if disease not found in mapping\n",
    "    disease_to_type[col] = types\n",
    "\n",
    "# Step 3: For each disease type, sum the corresponding disease columns\n",
    "for disease_type in disease_type_mapping.keys():\n",
    "    # Get all columns that map to this disease type\n",
    "    cols = [col for col, types in disease_to_type.items() if disease_type in types]\n",
    "    if cols:\n",
    "        # Sum the columns (since they are binary, this counts the number of diseases)\n",
    "        df[disease_type] = df[cols].sum(axis=1)\n",
    "    else:\n",
    "        df[disease_type] = 0  # If no diseases of this type are present\n",
    "\n",
    "# Step 4: Optionally, drop the individual disease columns\n",
    "df.drop(columns=disease_columns, inplace=True)\n",
    "\n",
    "df.to_csv('improving2.csv', index=False)\n",
    "\n",
    "# Now, df contains the non-disease columns and the summed disease type columns\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testando Melhores Variaveis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# Update the classifier in the pipeline to Gradient Boosting\n",
    "model_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', GradientBoostingClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "# Define parameter grid for Gradient Boosting tuning\n",
    "param_grid = {\n",
    "    'classifier__n_estimators': [100, 200],\n",
    "    'classifier__max_depth': [3, 5, 10],\n",
    "    'classifier__learning_rate': [0.01, 0.1, 0.2]\n",
    "}\n",
    "\n",
    "# Use GridSearchCV for hyperparameter tuning\n",
    "grid_search = GridSearchCV(model_pipeline, param_grid, cv=3, n_jobs=-1, verbose=2)\n",
    "\n",
    "# Fit the model\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = grid_search.best_estimator_.predict(X_test)\n",
    "print('Classification Report:\\n', classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "\n",
    "\n",
    "# Load the model later\n",
    "loaded_model = joblib.load('antibiotic_resistance_model.pkl')\n",
    "\n",
    "'''\n",
    "# Assuming new_data is a DataFrame with the same structure as your original X\n",
    "new_predictions = loaded_model.predict(new_data)\n",
    "\n",
    "# View the predictions\n",
    "print(new_predictions)\n",
    "'''\n",
    "X_train.columns\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
