{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INGESTÃO DE DADOS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "file_paths = {\n",
    "    'admissions': 'rawcsvs/ADMISSIONS.csv',\n",
    "    'microbiology_events': 'rawcsvs/MICROBIOLOGYEVENTS.csv',\n",
    "    'prescriptions': 'rawcsvs/PRESCRIPTIONS.csv',\n",
    "    'labevents': 'rawcsvs/LABEVENTS.csv',\n",
    "    'diagnoses_icd': 'rawcsvs/DIAGNOSES_ICD.csv',\n",
    "    'patients': 'rawcsvs/PATIENTS.csv',\n",
    "    'transl-labitems': 'rawcsvs/D_LABITEMS.csv',\n",
    "    'transl-diag': 'rawcsvs/D_ICD_DIAGNOSES.csv'\n",
    "}\n",
    "\n",
    "cols_to_use = {\n",
    "    'admissions': ['subject_id','admittime','ethnicity', 'admission_type'],\n",
    "    'microbiology_events': ['row_id', 'subject_id',  'chartdate', 'charttime', 'spec_itemid', 'spec_type_desc', \n",
    "                            'org_itemid', 'org_name', 'isolate_num', 'ab_itemid', 'ab_name', 'dilution_text', \n",
    "                            'dilution_comparison', 'dilution_value', 'interpretation'],\n",
    "    'prescriptions': ['subject_id', \"hadm_id\", 'drug', 'enddate'],\n",
    "    'labevents': ['subject_id', 'itemid', 'valuenum', 'valueuom', 'flag'],\n",
    "    'diagnoses_icd': ['subject_id', 'icd9_code'], #gotta work on this one, adding all codes to one line in the array\n",
    "    'transl-labitems': ['itemid', 'label'], # use this in conjunction with labevents to understando wtf if happening\n",
    "    'patients': ['subject_id', 'gender'],\n",
    "    'transl-diag': ['icd9_code', 'short_title', 'long_title']\n",
    "}\n",
    "\n",
    "files = {}\n",
    "for file_path in file_paths:\n",
    "    files[file_path] = pd.read_csv(file_paths[file_path], usecols=cols_to_use[file_path])\n",
    "    \n",
    "for file_path, file in files.items(): \n",
    "    if  not file_path.startswith('transl'):\n",
    "        file.dropna(inplace=True, subset=['subject_id'])\n",
    "        file.drop_duplicates(inplace=True)  \n",
    "        file_name = os.path.basename(file_path)\n",
    "        file.to_csv(f'cleanedcsv/{file_name}.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MANEIRA PARA JUNTAR TODOS OS DADOS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Isso esta feito para essas duas tabelas\n",
    "o modo nao mudou, precisa mudar as tabelas importadas, as colunas pivoteadas, e o valor de value, dependendo do que \n",
    "precisa ser pivoteado\n",
    "Agradeco Pedro por me dar a idea de pivotear as tabelas\n",
    "'''\n",
    "import pandas as pd\n",
    "mergedata = pd.read_csv('mergedcsv/admin-diag-lab-micro.csv')\n",
    "diagnoses = pd.read_csv('cleanedcsv/prescriptions.csv')\n",
    "\n",
    "\n",
    "#inverter a tabela de diagnósticos para ter uma linha por paciente\n",
    "\n",
    "diaginv = diagnoses.assign(value=1).pivot_table(\n",
    "    index='subject_id', \n",
    "    columns='drug', \n",
    "    values='value', \n",
    "    fill_value=0\n",
    ").reset_index()    # Reset the index to make `subject_id` a column\n",
    "\n",
    "diaginv.fillna(0, inplace=True)\n",
    "\n",
    "# Now proceed with the merge\n",
    "mergedata = mergedata.merge(diaginv, on='subject_id', how='left')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TRADUCAO DE CODIGOS PARA TEXTOS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#Need to convert the icd9_codes into text\n",
    "def tryconvertint(possibleint):\n",
    "    try:\n",
    "        x = int(possibleint)\n",
    "        return x\n",
    "    except ValueError:\n",
    "        return possibleint \n",
    "\n",
    "#first, I will get a representation of all the possible codes and their respective values\n",
    "unique_diag_codes = pd.read_csv('rawcsvs\\D_ICD_DIAGNOSES.csv').filter(['icd9_code', 'short_title']).drop_duplicates()\n",
    "unique_labitem_codes = pd.read_csv('rawcsvs\\D_LABITEMS.csv').filter(['itemid', 'label']).drop_duplicates()\n",
    "\n",
    "#second, I will iterate through each column name and replace it with the title and/or label\n",
    "allthedata = pd.read_csv('mergedcsv\\\\admin-diag-lab-micro-pat-prescrip.csv')\n",
    "\n",
    "new_columns = []\n",
    "\n",
    "for column in allthedata.columns:\n",
    "    colint = tryconvertint(column)\n",
    "    if colint == 50800:\n",
    "        x = unique_labitem_codes['itemid'].values\n",
    "        pass\n",
    "\n",
    "    # Check if the column is in the 'icd9_code' of unique_diag_codes\n",
    "    if column in unique_diag_codes['icd9_code'].values:\n",
    "        # Map it to 'short_title'\n",
    "        new_columns.append(unique_diag_codes.loc[unique_diag_codes['icd9_code'] == column, 'short_title'].values[0])\n",
    "\n",
    "    # Check if the column is in the 'itemid' of unique_labitem_codes\n",
    "    elif colint in unique_labitem_codes['itemid'].values:\n",
    "        # Map it to 'label'\n",
    "        new_columns.append(unique_labitem_codes.loc[unique_labitem_codes['itemid'] == colint, 'label'].values[0])\n",
    "\n",
    "    # If not found, retain the original column name\n",
    "    else:\n",
    "        new_columns.append(column)\n",
    "\n",
    "allthedata.columns = new_columns\n",
    "\n",
    "allthedata.to_csv('mergedcsv/final_merged_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_drugs = files['microbiology_events']['ab_name'].dropna().unique()\n",
    "all_drugs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EXTRACAO DE DADOS IMPORTANTES A BUSCA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#o negocio do Levenshtein ta qubrado, entao criei um customizado\n",
    "def levenshtein_distance(s1, s2):\n",
    "    if len(s1) < len(s2):\n",
    "        return levenshtein_distance(s2, s1)\n",
    "\n",
    "    # Initialize distance matrix\n",
    "    previous_row = range(len(s2) + 1)\n",
    "    \n",
    "    for i, c1 in enumerate(s1):\n",
    "        current_row = [i + 1]\n",
    "        for j, c2 in enumerate(s2):\n",
    "            insertions = previous_row[j + 1] + 1\n",
    "            deletions = current_row[j] + 1\n",
    "            substitutions = previous_row[j] + (c1 != c2)\n",
    "            current_row.append(min(insertions, deletions, substitutions))\n",
    "        previous_row = current_row\n",
    "    \n",
    "    return previous_row[-1]\n",
    "\n",
    "# Example search query\n",
    "\n",
    "search = 'fuck you you motherfuckerss'\n",
    "search = search.lower()  # Normalize case of the search term\n",
    "all_drugs = files['microbiology_events']['ab_name'].dropna().unique()\n",
    "\n",
    "closest_drug = None\n",
    "for drug in all_drugs:\n",
    "    dist = levenshtein_distance(search, drug.lower())  # Normalize case of drug names\n",
    "    if closest_drug is None or dist < closest_drug[1]:\n",
    "        closest_drug = (drug, dist)\n",
    "\n",
    "# Return just the drug name\n",
    "closest_drug_name = closest_drug[0]\n",
    "closest_drug_name\n",
    "\n",
    "\n",
    "#agora que temos o medicamento mais provavel, podemos listar as resistencias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coletando os dados de resistencia\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "#need to sort he diseases\n",
    "closest_drug = files['microbiology_events']['ab_name'].mode()[0]\n",
    "diseases = files['microbiology_events'][files['microbiology_events']['ab_name'] == closest_drug[0]]['org_name'].unique()\n",
    "\n",
    "\n",
    "\n",
    "dis_res_count = {}\n",
    "for disease in diseases:\n",
    "    disease_res = files['microbiology_events'][\n",
    "        (files['microbiology_events']['ab_name'] == closest_drug[0]) & \n",
    "        (files['microbiology_events']['org_name'] == disease)\n",
    "    ]['interpretation']\n",
    "    \n",
    "    # Initialize counts for 'R', 'I', 'S'\n",
    "    counts = {'R': 0, 'I': 0, 'S': 0}\n",
    "    \n",
    "    counts.update(disease_res.value_counts().to_dict())\n",
    "    \n",
    "    dis_res_count[disease] = counts\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FODASSE O MODELO PREDITIVO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, render_template, request, jsonify, redirect, url_for, session\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "app = Flask(__name__)\n",
    "app.secret_key = 'VAI SE FUDER FIGA' #desculpe bita cabrita\n",
    "\n",
    "@app.route('/', methods=['GET', 'POST'])\n",
    "def home():\n",
    "    if request.method == 'GET':\n",
    "        return render_template('busca.html')\n",
    "    elif request.method == 'POST':\n",
    "        ab_name = request.form.get('ab_name')\n",
    "\n",
    "        def search(term):\n",
    "            all_drugs = files['microbiology_events']['ab_name'].dropna().unique()\n",
    "\n",
    "            closest_drug = None    \n",
    "            for drug in all_drugs:\n",
    "                dist = levenshtein_distance(term, drug)\n",
    "                if closest_drug is None or dist < closest_drug[1]:\n",
    "                    closest_drug = (drug, dist)\n",
    "            return closest_drug\n",
    "        \n",
    "\n",
    "        if ab_name:\n",
    "            droga = search(ab_name)\n",
    "            session['droga'] = droga[0]  # Armazena apenas o nome da droga na sessão\n",
    "            return redirect(url_for('resposta'))\n",
    "        else:\n",
    "            return jsonify({\"error\": \"Nome do antibiótico não informado\"}), 400\n",
    "\n",
    "# Rota para a página secundaria (resposta)\n",
    "\n",
    "@app.route('/resposta')\n",
    "def resposta():\n",
    "    closest_drug = session.get('droga')  # Recupera a droga da sessão\n",
    "\n",
    "    # Verifica se o medicamento foi encontrado\n",
    "    if closest_drug:\n",
    "        # Chama a função que calcula as contagens de resistência\n",
    "        resistencias = dis_res_count(closest_drug)\n",
    "    else:\n",
    "        resistencias = {}\n",
    "\n",
    "    # Renderiza o template com os dados de resistência\n",
    "    return render_template('resposta.html', closest_drug=closest_drug, resistencias=resistencias)\n",
    "\n",
    "\n",
    "@app.route('/predict', methods=['POST'])\n",
    "def run_prediction():\n",
    "    if request.method == \"POST\":\n",
    "        byte_string = request.data.decode('utf-8')\n",
    "        data_dict = {item['name']: item['value'] for item in json.loads(byte_string)}        \n",
    "        \n",
    "        # Now we numericalize the data and test the model\n",
    "        data = pd.DataFrame(data_dict, index=[0])\n",
    "        selected_columns = [\"Cardiovascular\", \"Respiratory\", \"Neurological\", \"Infectious\", \n",
    "                        \"Gastrointestinal\", \"Renal\", \"Endocrine\", \"Oncology\", \"Trauma\", \"Other\"]\n",
    "        \n",
    "        data[selected_columns] = data[selected_columns].replace({'Low': 0, 'Medium': 1, 'High': 2})\n",
    "        \n",
    "        models, label_encoders = pickle.load(open('model.pkl', 'rb'))    \n",
    "        \n",
    "        for column, encoder in label_encoders.items():\n",
    "            data[column] = encoder.transform(data[column])\n",
    "            \n",
    "        predictions = []\n",
    "        for classifier in models:\n",
    "            prediction = classifier.predict(data)\n",
    "            predictions.append(prediction)\n",
    "            \n",
    "        score = sum(pred == 1 for pred in predictions)\n",
    "        percent_score = (score / len(predictions)) * 100  \n",
    "        \n",
    "        \n",
    "        return jsonify({'score': percent_score[0] }), 200\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run() \n",
    "\n",
    "\n",
    "def dis_res_count(closest_drug):\n",
    "    # Filtra as doenças baseadas no medicamento mais próximo\n",
    "    diseases = files['microbiology_events'][files['microbiology_events']['ab_name'] == closest_drug]['org_name'].unique()\n",
    "    \n",
    "    # Dicionário para armazenar a contagem de resistência por doença\n",
    "    dis_res_count = {}\n",
    "\n",
    "    # Itera sobre cada doença e conta as interpretações 'R', 'I', 'S'\n",
    "    for disease in diseases:\n",
    "        disease_res = files['microbiology_events'][\n",
    "            (files['microbiology_events']['ab_name'] == closest_drug) & \n",
    "            (files['microbiology_events']['org_name'] == disease)\n",
    "        ]['interpretation']\n",
    "        \n",
    "        # Inicializa a contagem de 'R', 'I', 'S'\n",
    "        counts = {'Resistence': 0, 'I': 0, 'S': 0}\n",
    "        \n",
    "        # Atualiza as contagens com base nas interpretações\n",
    "        counts.update(disease_res.value_counts().to_dict())\n",
    "        \n",
    "        # Armazena a contagem da doença atual\n",
    "        dis_res_count[disease] = counts\n",
    "\n",
    "    return dis_res_count\n",
    "\n",
    "def plot_resistance_graph(resistencias)\n",
    "\n",
    "# Dados para o gráfico\n",
    "data = {}\n",
    "for ab_name in all_drugs:\n",
    "    #precisa achar doencas cm esse ab\n",
    "    all_diseases = files['microbiology_events'].filter([\"ab_name\",\"org_name\"])\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Criando o gráfico de sunburst\n",
    "fig = px.sunburst(\n",
    "    data,\n",
    "    names='character',\n",
    "    parents='parent',\n",
    "    values='value',\n",
    "    color='character',  # Usa a cor com base nos nomes\n",
    "    color_discrete_map={'R': 'red', 'I': 'yellow', 'S': 'green'}  # Define as cores desejadas\n",
    ")\n",
    "\n",
    "# Atualizando o layout\n",
    "fig.update_layout(\n",
    "    title=f'Resistência para {session.get(\"droga\")}',\n",
    "    template='plotly_dark'\n",
    ")\n",
    "\n",
    "# Exibindo o gráfico\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Criando o dataset para o Modelo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os \n",
    "\n",
    "# Read data\n",
    "patients = pd.read_csv(os.path.join('rawcsvs', 'PATIENTS.csv'))\n",
    "admissions = pd.read_csv(os.path.join('rawcsvs', 'ADMISSIONS.csv'))\n",
    "microbiology = pd.read_csv(os.path.join('rawcsvs', 'MICROBIOLOGYEVENTS.csv'))\n",
    "\n",
    "# Merge the patients and admissions dataframes  \n",
    "new_data = (admissions\n",
    "            .filter(['subject_id', 'admission_type', 'ethnicity', 'diagnosis', 'hospital_expire_flag', 'insurance', 'religion', 'marital_status'])\n",
    "            .merge(patients, on='subject_id', how='left')\n",
    "            )\n",
    "\n",
    "# Merge the microbiology data and overwrite 'interpretation' with one-hot encoding\n",
    "new_data = (\n",
    "    microbiology\n",
    "    .filter(['subject_id', 'ab_name', 'interpretation'])\n",
    "    # One hot encoding the interpretation and overwriting the original 'interpretation' column\n",
    "    .assign(interpretation=(microbiology['interpretation'] == 'R').astype(int))\n",
    "    .merge(new_data, on='subject_id', how='left')\n",
    ")\n",
    "\n",
    "# Pivot diagnosis data for wide format\n",
    "aux1 = (\n",
    "    new_data\n",
    "    .filter(['subject_id', 'diagnosis'])\n",
    "    .assign(value=1)\n",
    "    .pivot_table(index='subject_id', columns='diagnosis', values='value', fill_value=0)\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Grouping the remaining columns\n",
    "aux2 = (\n",
    "    new_data\n",
    "    .groupby('subject_id')\n",
    "    .agg({\n",
    "        'insurance': 'first',\n",
    "        'religion': 'first',\n",
    "        'marital_status': 'first',\n",
    "        'ethnicity': 'first',\n",
    "        'gender': 'first',\n",
    "        'ab_name': 'first',\n",
    "        'interpretation': 'first'  # Now this is the one-hot encoded interpretation\n",
    "    })\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Merge aux1 and aux2 to get the final dataset\n",
    "filtered_data = aux1.merge(aux2, on='subject_id', how='left')\n",
    "\n",
    "# Export the final filtered data\n",
    "filtered_data.to_csv('improving.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Engineering (Fuck data science)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "filtered_data = pd.read_csv('improving.csv', sep=',')\n",
    "\n",
    "# Step 1: Identify non-disease columns\n",
    "non_disease_columns = ['subject_id', 'insurance', 'religion', 'marital_status', 'ethnicity', 'gender', 'ab_name', 'interpretation'] \n",
    "\n",
    "disease_columns = [col for col in filtered_data.columns if col not in non_disease_columns]\n",
    "\n",
    "# Step 2: Create a mapping from each disease to disease types\n",
    "disease_type_mapping = {\n",
    "    'Cardiovascular': ['ACUTE PULMONARY EMBOLISM', 'CONGESTIVE HEART FAILURE', 'CORONARY ARTERY DISEASE', 'STEMI', 'MI CHF', 'BRADYCARDIA', 'VF ARREST', 'PERICARDIAL EFFUSION', 'PULMONARY EDEMA, MI', 'PULMONARY EDEMA\\\\CATH', 'CRITICAL AORTIC STENOSIS/HYPOTENSION', 'INFERIOR MYOCARDIAL INFARCTION\\\\CATH'],\n",
    "    'Respiratory': ['ACUTE RESPIRATORY DISTRESS SYNDROME', 'ASTHMA', 'COPD FLARE', 'PNEUMONIA', 'RESPIRATORY DISTRESS', 'SHORTNESS OF BREATH', 'HYPOXIA', 'TACHYPNEA', 'PLEURAL EFFUSION', 'MEDIASTINAL ADENOPATHY'],\n",
    "    'Neurological': ['ALTERED MENTAL STATUS', 'SEIZURE', 'STATUS EPILEPTICUS', 'STROKE/TIA', 'INTRACRANIAL HEMORRHAGE', 'HEADACHE', 'FACIAL NUMBNESS', 'BASAL GANGLIN BLEED', 'ALTERED MENTAL STATUS'],\n",
    "    'Infectious': ['CELLULITIS', 'FEVER', 'SEPSIS', 'URINARY TRACT INFECTION', 'UROSEPSIS', 'PYELONEPHRITIS', 'UTI', 'HIV', 'ABSCESS', 'PNEUMONIA;TELEMETRY', 'SEPSIS; UTI', 'FEVER;URINARY TRACT INFECTION'],\n",
    "    'Gastrointestinal': ['ABDOMINAL PAIN', 'GASTROINTESTINAL BLEED', 'LOWER GI BLEED', 'UPPER GI BLEED', 'VARICEAL BLEED', 'HEPATITIS', 'LIVER FAILURE', 'CHOLECYSTITIS', 'CHOLANGITIS', 'PANCREATITIS', 'ACUTE CHOLECYSTITIS', 'ACUTE CHOLANGITIS', 'HEPATITIS B', 'ALCOHOLIC HEPATITIS', 'ELEVATED LIVER FUNCTIONS;S/P LIVER TRANSPLANT', 'FAILURE TO THRIVE'],\n",
    "    'Renal': ['ACUTE RENAL FAILURE', 'CHRONIC KIDNEY DISEASE', 'RENAL CANCER', 'HYPOTENSION, RENAL FAILURE'],\n",
    "    'Endocrine': ['DIABETES', 'HYPOGLYCEMIA', 'HYPERGLYCEMIA', 'HYPONATREMIA'],\n",
    "    'Oncology': ['LUNG CANCER', 'BREAST CANCER', 'COLON CANCER', 'LEUKEMIA', 'LYMPHOMA', 'METASTATIC MELANOMA', 'NON SMALL CELL CANCER', 'METASTIC MELANOMA;ANEMIA', 'CHRONIC MYELOGENOUS LEUKEMIA;TRANSFUSION REACTION'],\n",
    "    'Trauma': ['FRACTURE', 'MOTOR VEHICLE ACCIDENT', 'SUBDURAL HEMATOMA', 'LEFT HIP FRACTURE', 'RIGHT HUMEROUS FRACTURE', 'HUMERAL FRACTURE', 'S/P MOTOR VEHICLE ACCIDENT', 'ACUTE SUBDURAL HEMATOMA'],\n",
    "    'Other': ['OVERDOSE', 'FAILURE TO THRIVE', 'MEDIASTINAL ADENOPATHY', 'ARREST', 'OVERDOSE', 'AROMEGLEY;BURKITTS LYMPHOMA', 'TRACHEAL ESOPHAGEAL FISTULA', 'TRACHEAL STENOSIS', 'VOLVULUS', 'ANEMIA', 'BURKITTS LYMPHOMA', 'NON SMALL CELL CANCER;HYPOXIA', 'METASTATIC MELANOMA;BRAIN METASTASIS']\n",
    "}\n",
    "\n",
    "# Initialize a dictionary to map each disease column to its disease type(s)\n",
    "disease_to_type = {}\n",
    "\n",
    "# Process each disease column\n",
    "for col in disease_columns:\n",
    "    # Split disease names if they contain multiple diseases\n",
    "    diseases = re.split(';|,|/', col)\n",
    "    diseases = [disease.strip().upper() for disease in diseases]\n",
    "    types = set()\n",
    "    for disease in diseases:\n",
    "        # Find the disease type for this disease\n",
    "        found = False\n",
    "        for disease_type, disease_list in disease_type_mapping.items():\n",
    "            if disease in disease_list:\n",
    "                types.add(disease_type)\n",
    "                found = True\n",
    "        if not found:\n",
    "            types.add('Other')  # Assign 'Other' if disease not found in mapping\n",
    "    disease_to_type[col] = types\n",
    "\n",
    "\n",
    "# Step 3: For each disease type, sum the corresponding disease columns\n",
    "for disease_type in disease_type_mapping.keys():\n",
    "    # Get all columns that map to this disease type\n",
    "    cols = [col for col, types in disease_to_type.items() if disease_type in types]\n",
    "    if cols:\n",
    "        # Sum the columns (since they are binary, this counts the number of diseases)\n",
    "        filtered_data[disease_type] = filtered_data[cols].sum(axis=1)\n",
    "    else:\n",
    "        filtered_data[disease_type] = 0  # If no diseases of this type are present\n",
    "\n",
    "filtered_data.drop(columns=disease_columns, inplace=True)\n",
    "filtered_data.drop(columns=['subject_id'], inplace=True)     \n",
    "\n",
    "filtered_data.to_csv('improving2.csv', index=False)\n",
    "\n",
    "# Now, filtered_data contains the non-disease columns and the summed disease type columns\n",
    "print(filtered_data.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testando o Modelo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Load your CSV file\n",
    "df = pd.read_csv('improving2.csv')\n",
    "\n",
    "# Drop rows with missing target (interpretation)\n",
    "df.dropna(subset=['interpretation'], inplace=True)\n",
    "\n",
    "# Select relevant features and target\n",
    "features = ['insurance', 'religion', 'marital_status', 'ethnicity', 'gender', 'ab_name',\n",
    "            'Cardiovascular', 'Respiratory', 'Neurological', 'Infectious', 'Gastrointestinal', 'Renal',\n",
    "            'Endocrine', 'Oncology', 'Trauma', 'Other']\n",
    "target = 'interpretation'\n",
    "\n",
    "# Encode categorical features and target\n",
    "label_encoders = {}\n",
    "for column in features + [target]:\n",
    "    if df[column].dtype == 'object':\n",
    "        le = LabelEncoder()\n",
    "        df[column] = le.fit_transform(df[column])\n",
    "        label_encoders[column] = le\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X = df[features]\n",
    "y = df[target]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Apply SMOTE to balance the dataset\n",
    "smote = SMOTE(random_state=42, k_neighbors=1)\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# List of classifiers to test\n",
    "classifiers = {\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced'),\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42, class_weight='balanced'),\n",
    "    'SVM': SVC(random_state=42, class_weight='balanced'),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(random_state=42),\n",
    "    'KNN': KNeighborsClassifier()\n",
    "}\n",
    "\n",
    "# Train and evaluate each classifier\n",
    "for name, clf in classifiers.items():\n",
    "    print(f\"\\n{name} Results:\")\n",
    "    \n",
    "    # Train the classifier\n",
    "    clf.fit(X_train_smote, y_train_smote)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = clf.predict(X_test)\n",
    "    \n",
    "    # Evaluate the classifier\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Accuracy: {accuracy:.2f}\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred, zero_division=1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aprimorando Mais o Modelo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "# Como tem poucos 1s e muitos 0s, vamos usar o RandomOverSampler para balancear o dataset\n",
    "ros = RandomOverSampler(random_state=42)\n",
    "X_train_resampled, y_train_resampled = ros.fit_resample(X_train, y_train)\n",
    "\n",
    "\n",
    "# Train and evaluate each classifier\n",
    "for name, clf in classifiers.items():\n",
    "    print(f\"\\n{name} Results:\")\n",
    "    \n",
    "    # Train the classifier\n",
    "    clf.fit(X_train_resampled, y_train_resampled)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = clf.predict(X_test)\n",
    "    \n",
    "    # Evaluate the classifier\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Accuracy: {accuracy:.2f}\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred, zero_division=1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Salvando o modelo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#primeiro, precisamos salvar a *Pipeline de Dados* \n",
    "\n",
    "\n",
    "import pickle   \n",
    "\n",
    "# Save the trained model to a file  \n",
    "with open('model.pkl', 'wb') as model_file:\n",
    "    pickle.dump((classifiers, label_encoders), model_file)   \n",
    "    \n",
    "'''\n",
    "idea: salva todos e quando o usuario enviar os dados para o site, agente testa em todos \n",
    "e retorna uma media ponderada, considerando a taxa de acerto de cada um como o grau de conficanca\n",
    "\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
