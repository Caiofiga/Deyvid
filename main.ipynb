{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ingestão de dados\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "#criando dicionário para facilitar o nome dos arquivos \n",
    "file_paths = {\n",
    "    'admissions': 'rawcsvs/ADMISSIONS.csv',\n",
    "    'microbiology_events': 'rawcsvs/MICROBIOLOGYEVENTS.csv',\n",
    "    'prescriptions': 'rawcsvs/PRESCRIPTIONS.csv',\n",
    "    'labevents': 'rawcsvs/LABEVENTS.csv',\n",
    "    'diagnoses_icd': 'rawcsvs/DIAGNOSES_ICD.csv',\n",
    "    'patients': 'rawcsvs/PATIENTS.csv',\n",
    "    'transl-labitems': 'rawcsvs/D_LABITEMS.csv',\n",
    "    'transl-diag': 'rawcsvs/D_ICD_DIAGNOSES.csv'\n",
    "}\n",
    "\n",
    "\n",
    "#carregando arquivos\n",
    "files = {}\n",
    "for file_path in file_paths:\n",
    "    files[file_path] = pd.read_csv(file_paths[file_path])\n",
    "    \n",
    "for file_path, file in files.items(): \n",
    "    if  not file_path.startswith('transl'):\n",
    "        file.dropna(inplace=True, subset=['subject_id'])\n",
    "        file.drop_duplicates(inplace=True)  \n",
    "        file_name = os.path.basename(file_path)\n",
    "        file.to_csv(f'cleanedcsv/{file_name}.csv', index=False) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Criando o dataset para o Modelo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os \n",
    "\n",
    "# Lendo os dados dos arrquivos CSV\n",
    "patients = pd.read_csv(os.path.join('rawcsvs', 'PATIENTS.csv'))\n",
    "admissions = pd.read_csv(os.path.join('rawcsvs', 'ADMISSIONS.csv'))\n",
    "microbiology = pd.read_csv(os.path.join('rawcsvs', 'MICROBIOLOGYEVENTS.csv'))\n",
    "\n",
    "# Mesclando os dados de pacientes e admissões em um novo DataFrame chamado new_data\n",
    "new_data = (admissions\n",
    "            .filter(['subject_id', 'admission_type', 'ethnicity', 'diagnosis', 'hospital_expire_flag', 'insurance', 'religion', 'marital_status'])\n",
    "            .merge(patients, on='subject_id', how='left')\n",
    "            )\n",
    "\n",
    "# Mesclando os dados de microbiologia e aplicando one-hot encoding na coluna 'interpretation'\n",
    "new_data = (\n",
    "    microbiology\n",
    "    .filter(['subject_id', 'ab_name', 'interpretation'])\n",
    "    # One hot encoding the interpretation and overwriting the original 'interpretation' column\n",
    "    .assign(interpretation=(microbiology['interpretation'] == 'R').astype(int))\n",
    "    .merge(new_data, on='subject_id', how='left')\n",
    ")\n",
    "\n",
    "# Transformando os diagnósticos em formato 'wide' (largura)\n",
    "aux1 = (\n",
    "    new_data\n",
    "    .filter(['subject_id', 'diagnosis'])\n",
    "    .assign(value=1)\n",
    "    .pivot_table(index='subject_id', columns='diagnosis', values='value', fill_value=0)\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Agrupando as colunas restantes por 'subject_id'\n",
    "aux2 = (\n",
    "    new_data\n",
    "    .groupby('subject_id')\n",
    "    .agg({\n",
    "        'insurance': 'first',\n",
    "        'religion': 'first',\n",
    "        'marital_status': 'first',\n",
    "        'ethnicity': 'first',\n",
    "        'gender': 'first',\n",
    "        'ab_name': 'first',\n",
    "        'interpretation': 'first'  # Now this is the one-hot encoded interpretation\n",
    "    })\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Mesclando aux1 e aux2 para o data set final em um data frame\n",
    "filtered_data = aux1.merge(aux2, on='subject_id', how='left')\n",
    "\n",
    "# Exportando os dados filtrados para o CSV\n",
    "filtered_data.to_csv('improving.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Engineering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Carrega o arquivo CSV 'improving.csv' em um DataFrame chamado filtered_data\n",
    "filtered_data = pd.read_csv('improving.csv', sep=',')\n",
    "\n",
    "# Passo 1: Identifica as colunas que não são relacionadas a doenças\n",
    "non_disease_columns = ['subject_id', 'insurance', 'religion', 'marital_status', 'ethnicity', 'gender', 'ab_name', 'interpretation'] \n",
    "# Cria uma lista de todas as colunas que são doenças (excluindo as colunas não relacionadas a doenças)\n",
    "disease_columns = [col for col in filtered_data.columns if col not in non_disease_columns]\n",
    "\n",
    "# Passo 2: Mapeia cada doença a seus tipos \n",
    "# Define um dicionário que mapeia categorias de doenças (ex: Cardiovascular, Respiratória, etc.) a listas de doenças específicas\n",
    "disease_type_mapping = {\n",
    "    'Cardiovascular': ['ACUTE PULMONARY EMBOLISM', 'CONGESTIVE HEART FAILURE', 'CORONARY ARTERY DISEASE', 'STEMI', 'MI CHF', 'BRADYCARDIA', 'VF ARREST', 'PERICARDIAL EFFUSION', 'PULMONARY EDEMA, MI', 'PULMONARY EDEMA\\\\CATH', 'CRITICAL AORTIC STENOSIS/HYPOTENSION', 'INFERIOR MYOCARDIAL INFARCTION\\\\CATH'],\n",
    "    'Respiratory': ['ACUTE RESPIRATORY DISTRESS SYNDROME', 'ASTHMA', 'COPD FLARE', 'PNEUMONIA', 'RESPIRATORY DISTRESS', 'SHORTNESS OF BREATH', 'HYPOXIA', 'TACHYPNEA', 'PLEURAL EFFUSION', 'MEDIASTINAL ADENOPATHY'],\n",
    "    'Neurological': ['ALTERED MENTAL STATUS', 'SEIZURE', 'STATUS EPILEPTICUS', 'STROKE/TIA', 'INTRACRANIAL HEMORRHAGE', 'HEADACHE', 'FACIAL NUMBNESS', 'BASAL GANGLIN BLEED', 'ALTERED MENTAL STATUS'],\n",
    "    'Infectious': ['CELLULITIS', 'FEVER', 'SEPSIS', 'URINARY TRACT INFECTION', 'UROSEPSIS', 'PYELONEPHRITIS', 'UTI', 'HIV', 'ABSCESS', 'PNEUMONIA;TELEMETRY', 'SEPSIS; UTI', 'FEVER;URINARY TRACT INFECTION'],\n",
    "    'Gastrointestinal': ['ABDOMINAL PAIN', 'GASTROINTESTINAL BLEED', 'LOWER GI BLEED', 'UPPER GI BLEED', 'VARICEAL BLEED', 'HEPATITIS', 'LIVER FAILURE', 'CHOLECYSTITIS', 'CHOLANGITIS', 'PANCREATITIS', 'ACUTE CHOLECYSTITIS', 'ACUTE CHOLANGITIS', 'HEPATITIS B', 'ALCOHOLIC HEPATITIS', 'ELEVATED LIVER FUNCTIONS;S/P LIVER TRANSPLANT', 'FAILURE TO THRIVE'],\n",
    "    'Renal': ['ACUTE RENAL FAILURE', 'CHRONIC KIDNEY DISEASE', 'RENAL CANCER', 'HYPOTENSION, RENAL FAILURE'],\n",
    "    'Endocrine': ['DIABETES', 'HYPOGLYCEMIA', 'HYPERGLYCEMIA', 'HYPONATREMIA'],\n",
    "    'Oncology': ['LUNG CANCER', 'BREAST CANCER', 'COLON CANCER', 'LEUKEMIA', 'LYMPHOMA', 'METASTATIC MELANOMA', 'NON SMALL CELL CANCER', 'METASTIC MELANOMA;ANEMIA', 'CHRONIC MYELOGENOUS LEUKEMIA;TRANSFUSION REACTION'],\n",
    "    'Trauma': ['FRACTURE', 'MOTOR VEHICLE ACCIDENT', 'SUBDURAL HEMATOMA', 'LEFT HIP FRACTURE', 'RIGHT HUMEROUS FRACTURE', 'HUMERAL FRACTURE', 'S/P MOTOR VEHICLE ACCIDENT', 'ACUTE SUBDURAL HEMATOMA'],\n",
    "    'Other': ['OVERDOSE', 'FAILURE TO THRIVE', 'MEDIASTINAL ADENOPATHY', 'ARREST', 'OVERDOSE', 'AROMEGLEY;BURKITTS LYMPHOMA', 'TRACHEAL ESOPHAGEAL FISTULA', 'TRACHEAL STENOSIS', 'VOLVULUS', 'ANEMIA', 'BURKITTS LYMPHOMA', 'NON SMALL CELL CANCER;HYPOXIA', 'METASTATIC MELANOMA;BRAIN METASTASIS']\n",
    "}\n",
    "\n",
    "# Inicializa um dicionário para mapear cada coluna de doença para seu(s) tipo(s) de doença\n",
    "disease_to_type = {}\n",
    "\n",
    "# Processa cada coluna de doença\n",
    "for col in disease_columns:\n",
    "    # Separa os nomes de doenças caso contenham múltiplas doenças\n",
    "    diseases = re.split(';|,|/', col)\n",
    "    diseases = [disease.strip().upper() for disease in diseases]\n",
    "    types = set()\n",
    "    for disease in diseases:\n",
    "         # Encontra o tipo de doença correspondente\n",
    "        found = False\n",
    "        for disease_type, disease_list in disease_type_mapping.items():\n",
    "            if disease in disease_list:\n",
    "                types.add(disease_type)\n",
    "                found = True\n",
    "        if not found:\n",
    "            types.add('Other')  #  Se a doença não for encontrada, classifica como 'Other'\n",
    "    disease_to_type[col] = types # Mapeia a coluna da doença para os tipos encontrados\n",
    "\n",
    "\n",
    "# Passo 3: Para cada tipo de doença, soma as colunas correspondentes de doenças\n",
    "for disease_type in disease_type_mapping.keys():\n",
    "    # Obtém todas as colunas que correspondem a esse tipo de doença\n",
    "    cols = [col for col, types in disease_to_type.items() if disease_type in types]\n",
    "    if cols:\n",
    "        # Soma as colunas (como elas são binárias, isso conta o número de doenças desse tipo)\n",
    "        filtered_data[disease_type] = filtered_data[cols].sum(axis=1)\n",
    "    else:\n",
    "        filtered_data[disease_type] = 0  # Se não houver doenças desse tipo, atribui 0\n",
    "\n",
    "# Remove as colunas originais de doenças do DataFrame\n",
    "filtered_data.drop(columns=disease_columns, inplace=True)\n",
    "# Remove a coluna 'subject_id', pois não será mais necessária para as análises a seguir\n",
    "filtered_data.drop(columns=['subject_id'], inplace=True)    \n",
    "\n",
    "# Salva o DataFrame filtrado em um novo arquivo CSV\n",
    "filtered_data.to_csv('improving2.csv', index=False)\n",
    "\n",
    "# o DataFrame filtered_data contém as colunas não relacionadas a doenças e as colunas somadas por tipo de doença\n",
    "print(filtered_data.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testando o Modelo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "#  Remove linhas onde o alvo 'interpretation' está ausente (NaN)\n",
    "df = pd.read_csv('improving2.csv')\n",
    "\n",
    "#  Remove linhas onde o alvo 'interpretation' está ausente (NaN)\n",
    "df.dropna(subset=['interpretation'], inplace=True)\n",
    "\n",
    "# Seleciona as colunas de características relevantes e a coluna alvo ('interpretation')\n",
    "features = ['insurance', 'religion', 'marital_status', 'ethnicity', 'gender', 'ab_name',\n",
    "            'Cardiovascular', 'Respiratory', 'Neurological', 'Infectious', 'Gastrointestinal', 'Renal',\n",
    "            'Endocrine', 'Oncology', 'Trauma', 'Other']\n",
    "target = 'interpretation'\n",
    "\n",
    "# Codifica as variáveis categóricas e o alvo para numérico\n",
    "label_encoders = {}\n",
    "for column in features + [target]:\n",
    "    if df[column].dtype == 'object':\n",
    "        le = LabelEncoder()\n",
    "        df[column] = le.fit_transform(df[column])\n",
    "        label_encoders[column] = le\n",
    "\n",
    "# Divide os dados em conjuntos de treino e teste \n",
    "X = df[features]\n",
    "y = df[target]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Aplica SMOTE para balancear as classes no conjunto de treino\n",
    "smote = SMOTE(random_state=42, k_neighbors=1)\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Lista de classificadores que serão testados\n",
    "classifiers = {\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced'),\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42, class_weight='balanced'),\n",
    "    'SVM': SVC(random_state=42, class_weight='balanced'),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(random_state=42),\n",
    "    'KNN': KNeighborsClassifier()\n",
    "}\n",
    "\n",
    "# Treina e avalia cada classificador\n",
    "for name, clf in classifiers.items():\n",
    "    print(f\"\\n{name} Results:\")\n",
    "    \n",
    "    # Treina o classificador no conjunto de treino balanceado (SMOTE\n",
    "    clf.fit(X_train_smote, y_train_smote)\n",
    "    \n",
    "    # Faz previsões no conjunto de teste\n",
    "    y_pred = clf.predict(X_test)\n",
    "    \n",
    "    # Avalia o classificador com a métrica de acurácia\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Accuracy: {accuracy:.2f}\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred, zero_division=1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aprimorando modelo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "# Como tem poucos 1s e muitos 0s, vamos usar o RandomOverSampler para balancear o dataset\n",
    "ros = RandomOverSampler(random_state=42)\n",
    "X_train_resampled, y_train_resampled = ros.fit_resample(X_train, y_train)\n",
    "\n",
    "\n",
    "# Train and evaluate each classifier\n",
    "for name, clf in classifiers.items():\n",
    "    print(f\"\\n{name} Results:\")\n",
    "    \n",
    "    # Train the classifier\n",
    "    clf.fit(X_train_resampled, y_train_resampled)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = clf.predict(X_test)\n",
    "    \n",
    "    # Evaluate the classifier\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Accuracy: {accuracy:.2f}\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred, zero_division=1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Salvando o modelo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#primeiro, precisamos salvar a *Pipeline de Dados* \n",
    "\n",
    "import pickle   \n",
    "\n",
    "# Save the trained model to a file  \n",
    "with open('model.pkl', 'wb') as model_file:\n",
    "    pickle.dump((classifiers, label_encoders), model_file)   \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Páginas HTML\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
      " * Running on http://127.0.0.1:5000\n",
      "\u001b[33mPress CTRL+C to quit\u001b[0m\n",
      "127.0.0.1 - - [25/Oct/2024 22:01:52] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [25/Oct/2024 22:01:52] \"GET /static/estilos.css HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [25/Oct/2024 22:01:52] \"\u001b[36mGET /static/busca.js HTTP/1.1\u001b[0m\" 304 -\n",
      "127.0.0.1 - - [25/Oct/2024 22:01:54] \"\u001b[32mPOST / HTTP/1.1\u001b[0m\" 302 -\n",
      "127.0.0.1 - - [25/Oct/2024 22:01:54] \"GET /resposta HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [25/Oct/2024 22:01:54] \"GET /resposta HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [25/Oct/2024 22:01:54] \"\u001b[36mGET /static/resposta.css HTTP/1.1\u001b[0m\" 304 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ENTEROBACTERIACEAE': {'R': 0, 'I': 0, 'S': 2}, 'ESCHERICHIA COLI': {'R': 0, 'I': 0, 'S': 6}, 'SERRATIA MARCESCENS': {'R': 0, 'I': 0, 'S': 1}, 'KLEBSIELLA PNEUMONIAE': {'R': 0, 'I': 0, 'S': 1}, 'KLEBSIELLA OXYTOCA': {'R': 0, 'I': 0, 'S': 1}, 'ACINETOBACTER BAUMANNII COMPLEX': {'R': 1, 'I': 2, 'S': 4}}\n",
      "{'ENTEROBACTERIACEAE': {'R': 0, 'I': 0, 'S': 2}, 'ESCHERICHIA COLI': {'R': 0, 'I': 0, 'S': 6}, 'SERRATIA MARCESCENS': {'R': 0, 'I': 0, 'S': 1}, 'KLEBSIELLA PNEUMONIAE': {'R': 0, 'I': 0, 'S': 1}, 'KLEBSIELLA OXYTOCA': {'R': 0, 'I': 0, 'S': 1}, 'ACINETOBACTER BAUMANNII COMPLEX': {'R': 1, 'I': 2, 'S': 4}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [25/Oct/2024 22:01:54] \"\u001b[36mGET /static/resposta.js HTTP/1.1\u001b[0m\" 304 -\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, render_template, request, jsonify, redirect, url_for, session\n",
    "import json\n",
    "import pickle\n",
    "import secrets\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "\n",
    "app = Flask(__name__)\n",
    "app.secret_key = secrets.token_urlsafe(16)\n",
    "\n",
    "def levenshtein_distance(s1, s2):\n",
    "    if len(s1) < len(s2):\n",
    "        return levenshtein_distance(s2, s1)\n",
    "\n",
    "    # Initialize distance matrix\n",
    "    previous_row = range(len(s2) + 1)\n",
    "    \n",
    "    for i, c1 in enumerate(s1):\n",
    "        current_row = [i + 1]\n",
    "        for j, c2 in enumerate(s2):\n",
    "            insertions = previous_row[j + 1] + 1\n",
    "            deletions = current_row[j] + 1\n",
    "            substitutions = previous_row[j] + (c1 != c2)\n",
    "            current_row.append(min(insertions, deletions, substitutions))\n",
    "        previous_row = current_row\n",
    "    \n",
    "    return previous_row[-1]\n",
    "\n",
    "def dis_res_count(closest_drug):\n",
    "    # Filtra as doenças baseadas no medicamento mais próximo\n",
    "    diseases = files['microbiology_events'][files['microbiology_events']['ab_name'] == closest_drug]['org_name'].unique()\n",
    "    \n",
    "    # Dicionário para armazenar a contagem de resistência por doença\n",
    "    dis_res_count = {}\n",
    "\n",
    "    # Itera sobre cada doença e conta as interpretações 'R', 'I', 'S'\n",
    "    for disease in diseases:\n",
    "        disease_res = files['microbiology_events'][\n",
    "            (files['microbiology_events']['ab_name'] == closest_drug) & \n",
    "            (files['microbiology_events']['org_name'] == disease)\n",
    "        ]['interpretation']\n",
    "        \n",
    "        # Inicializa a contagem de 'R', 'I', 'S'\n",
    "        counts = {'R': 0, 'I': 0, 'S': 0}\n",
    "        \n",
    "        # Atualiza as contagens com base nas interpretações\n",
    "        counts.update(disease_res.value_counts().to_dict())\n",
    "        \n",
    "        # Armazena a contagem da doença atual\n",
    "        dis_res_count[disease] = counts\n",
    "\n",
    "    return dis_res_count\n",
    "\n",
    "def plot_resistance_graph(resistencias, closest_drug):\n",
    "    # Prepare data for sunburst plot\n",
    "    data = {\n",
    "        'character': [],\n",
    "        'parent': [],\n",
    "        'value': [],\n",
    "        'ids': [],\n",
    "        'labels': [],\n",
    "    }\n",
    "\n",
    "    # Add the antibiotic as the root\n",
    "    data['character'].append(closest_drug)\n",
    "    data['ids'].append(closest_drug)\n",
    "    data['labels'].append(closest_drug)\n",
    "    data['parent'].append('')\n",
    "    data['value'].append(sum([sum(counts.values())\n",
    "                         for counts in resistencias.values()]))\n",
    "\n",
    "    # Fill data for each bacteria and their resistance counts\n",
    "    for bacteria, counts in resistencias.items():\n",
    "        # Add bacteria as a child of the antibiotic\n",
    "        data['ids'].append(bacteria)\n",
    "        data['labels'].append(bacteria)\n",
    "        data['character'].append(bacteria)\n",
    "        data['parent'].append(closest_drug)\n",
    "        data['value'].append(sum(counts.values()))\n",
    "\n",
    "        # Add resistance levels as children of each bacteria, with unique identifiers\n",
    "        for i, (interpretation, count) in enumerate(counts.items()):\n",
    "            if count > 0:\n",
    "                unique_interpretation = f\"{interpretation}_{bacteria}\"\n",
    "                data['ids'].append(unique_interpretation)\n",
    "                data['labels'].append(interpretation)\n",
    "                data['character'].append(unique_interpretation)\n",
    "                data['parent'].append(bacteria)\n",
    "                data['value'].append(count)\n",
    "\n",
    "    # Create the sunburst plot\n",
    "    fig = px.sunburst(\n",
    "        data,\n",
    "        ids='ids',          # Unique IDs\n",
    "        names='labels',      # Display names\n",
    "        parents='parent',   # Parent-child relationships using IDs\n",
    "        values='value',     # Values for each level\n",
    "        color='labels',      # Color based on label names\n",
    "        color_discrete_map={'R': 'red', 'I': 'yellow', 'S': 'green'},\n",
    "        height=800,\n",
    "        width=800\n",
    "    )\n",
    "\n",
    "    # Update layout for transparency and size\n",
    "    fig.update_layout(\n",
    "        template=None,\n",
    "        paper_bgcolor='rgba(0,0,0,0)',\n",
    "        plot_bgcolor='rgba(0,0,0,0)',\n",
    "    )\n",
    "\n",
    "    # Convert the plot to HTML\n",
    "    graph_html = pio.to_html(fig, full_html=False)\n",
    "    return graph_html\n",
    "\n",
    "\n",
    "@app.route('/', methods=['GET', 'POST'])\n",
    "def home():\n",
    "    if request.method == 'GET':\n",
    "        return render_template('busca.html')\n",
    "    elif request.method == 'POST':\n",
    "        ab_name = request.form.get('ab_name')\n",
    "\n",
    "        def search(term):\n",
    "            all_drugs = files['microbiology_events']['ab_name'].dropna().unique()\n",
    "\n",
    "            closest_drug = None    \n",
    "            for drug in all_drugs:\n",
    "                dist = levenshtein_distance(term, drug)\n",
    "                if closest_drug is None or dist < closest_drug[1]:\n",
    "                   closest_drug = (drug, dist)\n",
    "            return closest_drug\n",
    "        \n",
    "\n",
    "        if ab_name:\n",
    "            droga = search(ab_name)\n",
    "            session['droga'] = droga[0]  # Armazena apenas o nome da droga na sessão\n",
    "            return redirect(url_for('resposta'))\n",
    "        else:\n",
    "            return jsonify({\"error\": \"Nome do antibiótico não informado\"}), 400\n",
    "\n",
    "# Rota para a página secundaria (resposta)\n",
    "\n",
    "@app.route('/resposta')\n",
    "def resposta():\n",
    "    closest_drug = session.get('droga')  # Recupera a droga da sessão\n",
    "\n",
    "    # Verifica se o medicamento foi encontrado\n",
    "    if closest_drug:\n",
    "        # Chama a função que calcula as contagens de resistência\n",
    "        resistencias = dis_res_count(closest_drug)\n",
    "        graph_html = plot_resistance_graph(resistencias, closest_drug)\n",
    "        print(resistencias)\n",
    "    else:\n",
    "        resistencias = {}\n",
    "        graph_html = None\n",
    "\n",
    "    # Renderiza o template com os dados de resistência\n",
    "    return render_template('resposta.html', closest_drug=closest_drug, resistencias=resistencias, graph_html=graph_html)\n",
    "\n",
    "\n",
    "# Função para predição de resistência via AJAX\n",
    "@app.route('/predict', methods=['POST'])\n",
    "def run_prediction():\n",
    "    if request.method == \"POST\":\n",
    "        byte_string = request.data.decode('utf-8')\n",
    "        data_dict = {item['name']: item['value'] for item in json.loads(byte_string)}\n",
    "        \n",
    "        data = pd.DataFrame(data_dict, index=[0])\n",
    "        selected_columns = [\"Cardiovascular\", \"Respiratory\", \"Neurological\", \"Infectious\", \n",
    "                        \"Gastrointestinal\", \"Renal\", \"Endocrine\", \"Oncology\", \"Trauma\", \"Other\"]\n",
    "        \n",
    "        data[selected_columns] = data[selected_columns].replace({'Low': 0, 'Medium': 1, 'High': 2})\n",
    "\n",
    "        # Define unique values for each column\n",
    "        all_ins = files['admissions']['insurance'].dropna().unique()\n",
    "        all_rels = files['admissions']['religion'].dropna().unique()\n",
    "        all_mar = files['admissions']['marital_status'].dropna().unique()\n",
    "        all_eth = files['admissions']['ethnicity'].dropna().unique()\n",
    "        all_gend = files['patients']['gender'].dropna().unique()\n",
    "        all_drugs = files['microbiology_events']['ab_name'].dropna().unique()\n",
    "\n",
    "\n",
    "        col_to_all = {\n",
    "            'insurance': all_ins,\n",
    "            'religion': all_rels,\n",
    "            'marital_status': all_mar,\n",
    "            'ethnicity': all_eth,\n",
    "            'gender': all_gend,\n",
    "            'ab_name': all_drugs\n",
    "        }\n",
    "\n",
    "        # Columns we want to compare\n",
    "        text_columns = ['insurance', 'religion', 'marital_status', 'ethnicity', 'gender', 'ab_name']\n",
    "\n",
    "        closest_received_data = {}\n",
    "        #iterate over the received dataset, finding distances to the all dataset\n",
    "        for text_col in text_columns:\n",
    "            closest_match = None\n",
    "            closest_distance = float('inf')  # Start with a large number for initial comparison\n",
    "\n",
    "            for received_col in data[text_col]:  # iterates over received data\n",
    "                for all_col in col_to_all[text_col]:  # iterates over trained dataset data\n",
    "                    dist = levenshtein_distance(received_col, all_col)\n",
    "\n",
    "                    # Update if a closer match is found\n",
    "                    if dist < closest_distance:\n",
    "                        closest_match = all_col\n",
    "                        closest_distance = dist\n",
    "\n",
    "            # Store the closest match and its distance\n",
    "            closest_received_data[text_col] = closest_match\n",
    "        text_df = pd.DataFrame(closest_received_data, index=[0])\n",
    "        data_to_predict = pd.concat([data[selected_columns], text_df], axis=1).iloc[0:1]\n",
    "        data_to_predict = data_to_predict[[\n",
    "            'insurance', 'religion', 'marital_status', 'ethnicity', 'gender', 'ab_name',\n",
    "            'Cardiovascular', 'Respiratory', 'Neurological', 'Infectious', \n",
    "            'Gastrointestinal', 'Renal', 'Endocrine', 'Oncology', 'Trauma', 'Other'\n",
    "        ]].iloc[0:1]\n",
    "    \n",
    "\n",
    "        \n",
    "        models, label_encoders = pickle.load(open('model.pkl', 'rb'))\n",
    "\n",
    "        for column, encoder in label_encoders.items():\n",
    "            data_to_predict[column] = encoder.transform(data_to_predict[column])\n",
    "\n",
    "        predictions = []\n",
    "        for name, classifier in models.items():\n",
    "            prediction = classifier.predict(data_to_predict)\n",
    "            predictions.append(prediction)\n",
    "\n",
    "        score = sum(pred == 1 for pred in predictions)\n",
    "        percent_score = (score / len(predictions)) * 100\n",
    "\n",
    "        return jsonify({'score': percent_score[0]}), 200\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run() \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
