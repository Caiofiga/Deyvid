{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INGESTÃO DE DADOS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "file_paths = {\n",
    "    'admissions': 'rawcsvs/ADMISSIONS.csv',\n",
    "    'microbiology_events': 'rawcsvs/MICROBIOLOGYEVENTS.csv',\n",
    "    'prescriptions': 'rawcsvs/PRESCRIPTIONS.csv',\n",
    "    'labevents': 'rawcsvs/LABEVENTS.csv',\n",
    "    'diagnoses_icd': 'rawcsvs/DIAGNOSES_ICD.csv',\n",
    "    'patients': 'rawcsvs/PATIENTS.csv',\n",
    "    'transl-labitems': 'rawcsvs/D_LABITEMS.csv',\n",
    "    'transl-diag': 'rawcsvs/D_ICD_DIAGNOSES.csv'\n",
    "}\n",
    "\n",
    "cols_to_use = {\n",
    "    'admissions': ['subject_id','admittime','ethnicity', 'admission_type'],\n",
    "    'microbiology_events': ['row_id', 'subject_id',  'chartdate', 'charttime', 'spec_itemid', 'spec_type_desc', \n",
    "                            'org_itemid', 'org_name', 'isolate_num', 'ab_itemid', 'ab_name', 'dilution_text', \n",
    "                            'dilution_comparison', 'dilution_value', 'interpretation'],\n",
    "    'prescriptions': ['subject_id', \"hadm_id\", 'drug', 'enddate'],\n",
    "    'labevents': ['subject_id', 'itemid', 'valuenum', 'valueuom', 'flag'],\n",
    "    'diagnoses_icd': ['subject_id', 'icd9_code'], #gotta work on this one, adding all codes to one line in the array\n",
    "    'transl-labitems': ['itemid', 'label'], # use this in conjunction with labevents to understando wtf if happening\n",
    "    'patients': ['subject_id', 'gender'],\n",
    "    'transl-diag': ['icd9_code', 'short_title', 'long_title']\n",
    "}\n",
    "\n",
    "files = {}\n",
    "for file_path in file_paths:\n",
    "    files[file_path] = pd.read_csv(file_paths[file_path], usecols=cols_to_use[file_path])\n",
    "    \n",
    "for file_path, file in files.items(): \n",
    "    if  not file_path.startswith('transl'):\n",
    "        file.dropna(inplace=True, subset=['subject_id'])\n",
    "        file.drop_duplicates(inplace=True)  \n",
    "        file_name = os.path.basename(file_path)\n",
    "        file.to_csv(f'cleanedcsv/{file_name}.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MANEIRA PARA JUNTAR TODOS OS DADOS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Isso esta feito para essas duas tabelas\n",
    "o modo nao mudou, precisa mudar as tabelas importadas, as colunas pivoteadas, e o valor de value, dependendo do que \n",
    "precisa ser pivoteado\n",
    "Agradeco Pedro por me dar a idea de pivotear as tabelas\n",
    "'''\n",
    "import pandas as pd\n",
    "mergedata = pd.read_csv('mergedcsv/admin-diag-lab-micro.csv')\n",
    "diagnoses = pd.read_csv('cleanedcsv/prescriptions.csv')\n",
    "\n",
    "\n",
    "#inverter a tabela de diagnósticos para ter uma linha por paciente\n",
    "\n",
    "diaginv = diagnoses.assign(value=1).pivot_table(\n",
    "    index='subject_id', \n",
    "    columns='drug', \n",
    "    values='value', \n",
    "    fill_value=0\n",
    ").reset_index()    # Reset the index to make `subject_id` a column\n",
    "\n",
    "diaginv.fillna(0, inplace=True)\n",
    "\n",
    "# Now proceed with the merge\n",
    "mergedata = mergedata.merge(diaginv, on='subject_id', how='left')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TRADUCAO DE CODIGOS PARA TEXTOS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['subject_id', 'admittime', 'admission_type', 'ethnicity',\n",
       "       'Int inf clstrdium dfcile', 'Streptococcal septicemia',\n",
       "       'Meth susc Staph aur sept', 'MRSA septicemia',\n",
       "       'Staphylcocc septicem NEC', 'Anaerobic septicemia',\n",
       "       ...\n",
       "       'Zonisamide', 'donepezil 5 mg or placebo', 'sevelamer HYDROCHLORIDE',\n",
       "       'sodium bic', 'sodium bicar', 'sodium bicarb', 'traMADOL', 'traZODONE',\n",
       "       'traZODONE HCl', 'tucks'],\n",
       "      dtype='object', length=1662)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "#Need to convert the icd9_codes into text\n",
    "def tryconvertint(possibleint):\n",
    "    try:\n",
    "        x = int(possibleint)\n",
    "        return x\n",
    "    except ValueError:\n",
    "        return possibleint \n",
    "\n",
    "#first, I will get a representation of all the possible codes and their respective values\n",
    "unique_diag_codes = pd.read_csv(os.path.join('rawcsvs', 'D_ICD_DIAGNOSES.csv')).filter(['icd9_code', 'short_title']).drop_duplicates()\n",
    "unique_labitem_codes = pd.read_csv(os.path.join('rawcsvs', 'D_LABITEMS.csv')).filter(['itemid', 'label']).drop_duplicates()\n",
    "\n",
    "#second, I will iterate through each column name and replace it with the title and/or label\n",
    "allthedata = pd.read_csv(os.path.join('mergedcsv', 'admin-diag-lab-micro-pat-prescrip.csv'))\n",
    "\n",
    "new_columns = []\n",
    "\n",
    "for column in allthedata.columns:\n",
    "    colint = tryconvertint(column)\n",
    "    if colint == 50800:\n",
    "        x = unique_labitem_codes['itemid'].values\n",
    "        pass\n",
    "\n",
    "    # Check if the column is in the 'icd9_code' of unique_diag_codes\n",
    "    if column in unique_diag_codes['icd9_code'].values:\n",
    "        # Map it to 'short_title'\n",
    "        new_columns.append(unique_diag_codes.loc[unique_diag_codes['icd9_code'] == column, 'short_title'].values[0])\n",
    "\n",
    "    # Check if the column is in the 'itemid' of unique_labitem_codes\n",
    "    elif colint in unique_labitem_codes['itemid'].values:\n",
    "        # Map it to 'label'\n",
    "        new_columns.append(unique_labitem_codes.loc[unique_labitem_codes['itemid'] == colint, 'label'].values[0])\n",
    "\n",
    "    # If not found, retain the original column name\n",
    "    else:\n",
    "        new_columns.append(column)\n",
    "\n",
    "allthedata.columns = new_columns\n",
    "allthedata.columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EXTRACAO DE DADOS IMPORTANTES A BUSCA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Levenshtein\n",
    "#supunhetemos que tenhamos uma busca\n",
    "ab_name = 'vancomycin'  # Define a default value for ab_name\n",
    "search = ab_name if ab_name else 'vancomycin'\n",
    "#precisamos listar todas as doencas em que o medicamento foi usado\n",
    "#para facilitar a busca, precisamos calcular o medicamento mais provavel que foi escrito\n",
    "all_drugs = files['microbiology_events']['ab_name'].dropna().unique()\n",
    "\n",
    "closest_drug = None\t\n",
    "for drug in all_drugs:\n",
    "    dist = Levenshtein.distance(search, drug)\n",
    "    if closest_drug is None or dist < closest_drug[1]:\n",
    "        closest_drug = (drug, dist)\n",
    "\n",
    "#agora que temos o medicamento mais provavel, podemos listar as resistencias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coletando os dados de resistencia\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "#need to sort he diseases\n",
    "closest_drug = files['microbiology_events']['ab_name'].mode()[0]\n",
    "diseases = files['microbiology_events'][files['microbiology_events']['ab_name'] == closest_drug[0]]['org_name'].unique()\n",
    "\n",
    "\n",
    "\n",
    "dis_res_count = {}\n",
    "for disease in diseases:\n",
    "    disease_res = files['microbiology_events'][\n",
    "        (files['microbiology_events']['ab_name'] == closest_drug[0]) & \n",
    "        (files['microbiology_events']['org_name'] == disease)\n",
    "    ]['interpretation']\n",
    "    \n",
    "    # Initialize counts for 'R', 'I', 'S'\n",
    "    counts = {'R': 0, 'I': 0, 'S': 0}\n",
    "    \n",
    "    counts.update(disease_res.value_counts().to_dict())\n",
    "    \n",
    "    dis_res_count[disease] = counts\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Cannot index by location index with a non-integer key",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[37], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m all_diseases:\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m ab_name \u001b[38;5;129;01min\u001b[39;00m all_drugs:\n\u001b[1;32m----> 6\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m ab_name \u001b[38;5;241m==\u001b[39m \u001b[43mall_diseases\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m]\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mab_name\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[0;32m      7\u001b[0m             matches[ab_name] \u001b[38;5;241m=\u001b[39m all_diseases\u001b[38;5;241m.\u001b[39miloc[row][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124morg_name\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      8\u001b[0m matches\n",
      "File \u001b[1;32mc:\\Users\\bitan\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\indexing.py:1191\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1189\u001b[0m maybe_callable \u001b[38;5;241m=\u001b[39m com\u001b[38;5;241m.\u001b[39mapply_if_callable(key, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj)\n\u001b[0;32m   1190\u001b[0m maybe_callable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_deprecated_callable_usage(key, maybe_callable)\n\u001b[1;32m-> 1191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmaybe_callable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\bitan\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\indexing.py:1749\u001b[0m, in \u001b[0;36m_iLocIndexer._getitem_axis\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1747\u001b[0m key \u001b[38;5;241m=\u001b[39m item_from_zerodim(key)\n\u001b[0;32m   1748\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_integer(key):\n\u001b[1;32m-> 1749\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot index by location index with a non-integer key\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1751\u001b[0m \u001b[38;5;66;03m# validate the location\u001b[39;00m\n\u001b[0;32m   1752\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_integer(key, axis)\n",
      "\u001b[1;31mTypeError\u001b[0m: Cannot index by location index with a non-integer key"
     ]
    }
   ],
   "source": [
    "\n",
    "all_diseases = files['microbiology_events'].filter([\"ab_name\",\"org_name\"]).dropna()\n",
    "matches = {}\n",
    "for row in all_diseases:\n",
    "    for ab_name in all_drugs:\n",
    "\n",
    "        if ab_name == all_diseases.iloc[row]['ab_name']:\n",
    "            matches[ab_name] = all_diseases.iloc[row]['org_name']\n",
    "matches\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FODASSE O MODELO PREDITIVO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "expected ':' (2715529739.py, line 84)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[32], line 84\u001b[1;36m\u001b[0m\n\u001b[1;33m    def plot_resistance_graph(resistencias)\u001b[0m\n\u001b[1;37m                                           ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m expected ':'\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, render_template, request, jsonify, redirect, url_for, session\n",
    "import Levenshtein\n",
    "import plotly.express as px\n",
    "import plotly.graph_objs as go\n",
    "import plotly.io as pio\n",
    "\n",
    "\n",
    "app = Flask(__name__)\n",
    "app.secret_key = 'VAI SE FUDER MAMU' #perdao bita cabrita\n",
    "\n",
    "@app.route('/', methods=['GET', 'POST'])\n",
    "def home():\n",
    "    if request.method == 'GET':\n",
    "        return render_template('busca.html')\n",
    "    elif request.method == 'POST':\n",
    "        ab_name = request.form.get('ab_name')\n",
    "\n",
    "        def search(term):\n",
    "            all_drugs = files['microbiology_events']['ab_name'].dropna().unique()\n",
    "\n",
    "            closest_drug = None    \n",
    "            for drug in all_drugs:\n",
    "                dist = Levenshtein.distance(term, drug)\n",
    "                if closest_drug is None or dist < closest_drug[1]:\n",
    "                   closest_drug = (drug, dist)\n",
    "            return closest_drug\n",
    "        \n",
    "\n",
    "        if ab_name:\n",
    "            droga = search(ab_name)\n",
    "            session['droga'] = droga[0]  # Armazena apenas o nome da droga na sessão\n",
    "            return redirect(url_for('resposta'))\n",
    "        else:\n",
    "            return jsonify({\"error\": \"Nome do antibiótico não informado\"}), 400\n",
    "\n",
    "# Rota para a página secundaria (resposta)\n",
    "\n",
    "@app.route('/resposta')\n",
    "\n",
    "def resposta():\n",
    "    closest_drug = session.get('droga')  # Recupera a droga da sessão\n",
    "\n",
    "    # Verifica se o medicamento foi encontrado\n",
    "    if closest_drug:\n",
    "        # Chama a função que calcula as contagens de resistência\n",
    "        resistencias = dis_res_count(closest_drug)\n",
    "        graph_url = plot_resistance_graph(resistencias)\n",
    "    else:\n",
    "        resistencias = {}\n",
    "        graph_url = None\n",
    "      \n",
    "    # Renderiza o template com os dados de resistência\n",
    "    return render_template('resposta.html', closest_drug=closest_drug, resistencias=resistencias, graph_url=graph_url)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run() \n",
    "\n",
    "\n",
    "def dis_res_count(closest_drug):\n",
    "    # Filtra as doenças baseadas no medicamento mais próximo\n",
    "    diseases = files['microbiology_events'][files['microbiology_events']['ab_name'] == closest_drug]['org_name'].unique()\n",
    "    \n",
    "    # Dicionário para armazenar a contagem de resistência por doença\n",
    "    dis_res_count = {}\n",
    "\n",
    "    # Itera sobre cada doença e conta as interpretações 'R', 'I', 'S'\n",
    "    for disease in diseases:\n",
    "        disease_res = files['microbiology_events'][\n",
    "            (files['microbiology_events']['ab_name'] == closest_drug) & \n",
    "            (files['microbiology_events']['org_name'] == disease)\n",
    "        ]['interpretation']\n",
    "        \n",
    "        # Inicializa a contagem de 'R', 'I', 'S'\n",
    "        counts = {'Resistence': 0, 'I': 0, 'S': 0}\n",
    "        \n",
    "        # Atualiza as contagens com base nas interpretações\n",
    "        counts.update(disease_res.value_counts().to_dict())\n",
    "        \n",
    "        # Armazena a contagem da doença atual\n",
    "        dis_res_count[disease] = counts\n",
    "\n",
    "    return dis_res_count\n",
    "\n",
    "def plot_resistance_graph(resistencias)\n",
    "\n",
    "# Dados para o gráfico\n",
    "data = {}\n",
    "for ab_name in all_drugs:\n",
    "    #precisa achar doencas cm esse ab\n",
    "    all_diseases = files['microbiology_events'].filter([\"ab_name\",\"org_name\"])\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Criando o gráfico de sunburst\n",
    "fig = px.sunburst(\n",
    "    data,\n",
    "    names='character',\n",
    "    parents='parent',\n",
    "    values='value',\n",
    "    color='character',  # Usa a cor com base nos nomes\n",
    "    color_discrete_map={'R': 'red', 'I': 'yellow', 'S': 'green'}  # Define as cores desejadas\n",
    ")\n",
    "\n",
    "# Atualizando o layout\n",
    "fig.update_layout(\n",
    "    title=f'Resistência para {session.get(\"droga\")}',\n",
    "    template='plotly_dark'\n",
    ")\n",
    "\n",
    "# Exibindo o gráfico\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Criando o dataset para o Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os \n",
    "\n",
    "patients = pd.read_csv(os.path.join('rawcsvs', 'PATIENTS.csv'))\n",
    "admissions = pd.read_csv(os.path.join('rawcsvs', 'ADMISSIONS.csv'))\n",
    "microbiology = pd.read_csv(os.path.join('rawcsvs', 'MICROBIOLOGYEVENTS.csv'))\n",
    "\n",
    "\n",
    "# Merge the patients and admissions dataframes  \n",
    "new_data = (admissions\n",
    "            .filter(['subject_id', 'admission_type', 'ethnicity', 'diagnosis', 'hospital_expire_flag', 'insurance', 'religion', 'marital_status'])\n",
    "            #.drop(admissions[admissions['hospital_expire_flag'] == 1].index)\n",
    "            .merge(patients, on='subject_id', how='left')\n",
    "            )\n",
    "#merge the microbiology data\n",
    "new_data = (\n",
    "    microbiology\n",
    "    .filter(['subject_id', 'ab_name', 'interpretation'])\n",
    "    # One hot encoding the interpretation\n",
    "    .assign(value=(microbiology['interpretation'] == 'R').astype(int))\n",
    "    .reset_index(drop=True)\n",
    "    .merge(new_data, on='subject_id', how='left')\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "aux1 = ( # one hot enconding e passando para formato largo os diagnósticos\n",
    "    new_data\n",
    "    .filter(['subject_id', 'diagnosis'])\n",
    "    .assign(value=1)\n",
    "    .pivot_table(index='subject_id', columns='diagnosis', values='value', fill_value=0)\n",
    "    .reset_index()\n",
    ")\n",
    "aux2 = (\n",
    "    new_data\n",
    "    .groupby('subject_id')\n",
    "    .agg({\n",
    "        'insurance': 'first',\n",
    "        'religion': 'first',\n",
    "        'marital_status': 'first',\n",
    "        'ethnicity': 'first',\n",
    "        'gender': 'first',\n",
    "        'ab_name': 'first',\n",
    "        'interpretation': 'first'\n",
    "    })\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "filtered_data = aux1.merge(aux2, on='subject_id', how='left')\n",
    "filtered_data.to_csv('improving.csv', index=False)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Engineering (Fuck data science)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   subject_id insurance  religion marital_status               ethnicity  \\\n",
      "0       10006  Medicare  CATHOLIC      SEPARATED  BLACK/AFRICAN AMERICAN   \n",
      "1       10011   Private  CATHOLIC         SINGLE   UNKNOWN/NOT SPECIFIED   \n",
      "2       10013  Medicare  CATHOLIC            NaN   UNKNOWN/NOT SPECIFIED   \n",
      "3       10017  Medicare  CATHOLIC       DIVORCED                   WHITE   \n",
      "4       10019  Medicare  CATHOLIC       DIVORCED                   WHITE   \n",
      "\n",
      "  gender       ab_name interpretation  Cardiovascular  Respiratory  \\\n",
      "0      F    VANCOMYCIN              S             0.0          0.0   \n",
      "1      F           NaN            NaN             0.0          0.0   \n",
      "2      F           NaN            NaN             0.0          0.0   \n",
      "3      F           NaN            NaN             0.0          0.0   \n",
      "4      M  ERYTHROMYCIN              R             0.0          0.0   \n",
      "\n",
      "   Neurological  Infectious  Gastrointestinal  Renal  Endocrine  Oncology  \\\n",
      "0           0.0         1.0               0.0    0.0        0.0       0.0   \n",
      "1           0.0         0.0               1.0    0.0        0.0       0.0   \n",
      "2           0.0         1.0               0.0    0.0        0.0       0.0   \n",
      "3           0.0         0.0               0.0    0.0        0.0       0.0   \n",
      "4           0.0         0.0               1.0    0.0        0.0       0.0   \n",
      "\n",
      "   Trauma  Other  \n",
      "0     0.0    0.0  \n",
      "1     0.0    0.0  \n",
      "2     0.0    0.0  \n",
      "3     1.0    0.0  \n",
      "4     0.0    0.0  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Assuming your DataFrame is called 'df'\n",
    "df = pd.read_csv('improving.csv', sep=',')\n",
    "\n",
    "# Step 1: Identify non-disease columns\n",
    "non_disease_columns = ['subject_id', 'insurance', 'religion', 'marital_status', 'ethnicity', 'gender', 'ab_name', 'interpretation'] \n",
    "\n",
    "# Identify disease columns by excluding non-disease columns\n",
    "disease_columns = [col for col in df.columns if col not in non_disease_columns]\n",
    "\n",
    "# Step 2: Create a mapping from each disease to disease types\n",
    "# Define your disease types and map diseases to them\n",
    "disease_type_mapping = {\n",
    "    'Cardiovascular': ['ACUTE PULMONARY EMBOLISM', 'CONGESTIVE HEART FAILURE', 'CORONARY ARTERY DISEASE', 'STEMI', 'MI CHF', 'BRADYCARDIA', 'VF ARREST', 'PERICARDIAL EFFUSION', 'PULMONARY EDEMA, MI', 'PULMONARY EDEMA\\\\CATH', 'CRITICAL AORTIC STENOSIS/HYPOTENSION', 'INFERIOR MYOCARDIAL INFARCTION\\\\CATH'],\n",
    "    'Respiratory': ['ACUTE RESPIRATORY DISTRESS SYNDROME', 'ASTHMA', 'COPD FLARE', 'PNEUMONIA', 'RESPIRATORY DISTRESS', 'SHORTNESS OF BREATH', 'HYPOXIA', 'TACHYPNEA', 'PLEURAL EFFUSION', 'MEDIASTINAL ADENOPATHY'],\n",
    "    'Neurological': ['ALTERED MENTAL STATUS', 'SEIZURE', 'STATUS EPILEPTICUS', 'STROKE/TIA', 'INTRACRANIAL HEMORRHAGE', 'HEADACHE', 'FACIAL NUMBNESS', 'BASAL GANGLIN BLEED', 'ALTERED MENTAL STATUS'],\n",
    "    'Infectious': ['CELLULITIS', 'FEVER', 'SEPSIS', 'URINARY TRACT INFECTION', 'UROSEPSIS', 'PYELONEPHRITIS', 'UTI', 'HIV', 'ABSCESS', 'PNEUMONIA;TELEMETRY', 'SEPSIS; UTI', 'FEVER;URINARY TRACT INFECTION'],\n",
    "    'Gastrointestinal': ['ABDOMINAL PAIN', 'GASTROINTESTINAL BLEED', 'LOWER GI BLEED', 'UPPER GI BLEED', 'VARICEAL BLEED', 'HEPATITIS', 'LIVER FAILURE', 'CHOLECYSTITIS', 'CHOLANGITIS', 'PANCREATITIS', 'ACUTE CHOLECYSTITIS', 'ACUTE CHOLANGITIS', 'HEPATITIS B', 'ALCOHOLIC HEPATITIS', 'ELEVATED LIVER FUNCTIONS;S/P LIVER TRANSPLANT', 'FAILURE TO THRIVE'],\n",
    "    'Renal': ['ACUTE RENAL FAILURE', 'CHRONIC KIDNEY DISEASE', 'RENAL CANCER', 'HYPOTENSION, RENAL FAILURE'],\n",
    "    'Endocrine': ['DIABETES', 'HYPOGLYCEMIA', 'HYPERGLYCEMIA', 'HYPONATREMIA'],\n",
    "    'Oncology': ['LUNG CANCER', 'BREAST CANCER', 'COLON CANCER', 'LEUKEMIA', 'LYMPHOMA', 'METASTATIC MELANOMA', 'NON SMALL CELL CANCER', 'METASTIC MELANOMA;ANEMIA', 'CHRONIC MYELOGENOUS LEUKEMIA;TRANSFUSION REACTION'],\n",
    "    'Trauma': ['FRACTURE', 'MOTOR VEHICLE ACCIDENT', 'SUBDURAL HEMATOMA', 'LEFT HIP FRACTURE', 'RIGHT HUMEROUS FRACTURE', 'HUMERAL FRACTURE', 'S/P MOTOR VEHICLE ACCIDENT', 'ACUTE SUBDURAL HEMATOMA'],\n",
    "    'Other': ['OVERDOSE', 'FAILURE TO THRIVE', 'MEDIASTINAL ADENOPATHY', 'ARREST', 'OVERDOSE', 'AROMEGLEY;BURKITTS LYMPHOMA', 'TRACHEAL ESOPHAGEAL FISTULA', 'TRACHEAL STENOSIS', 'VOLVULUS', 'ANEMIA', 'BURKITTS LYMPHOMA', 'NON SMALL CELL CANCER;HYPOXIA', 'METASTATIC MELANOMA;BRAIN METASTASIS']\n",
    "}\n",
    "\n",
    "# Initialize a dictionary to map each disease column to its disease type(s)\n",
    "disease_to_type = {}\n",
    "\n",
    "# Process each disease column\n",
    "for col in disease_columns:\n",
    "    # Split disease names if they contain multiple diseases\n",
    "    diseases = re.split(';|,|/', col)\n",
    "    diseases = [disease.strip().upper() for disease in diseases]\n",
    "    types = set()\n",
    "    for disease in diseases:\n",
    "        # Find the disease type for this disease\n",
    "        found = False\n",
    "        for disease_type, disease_list in disease_type_mapping.items():\n",
    "            if disease in disease_list:\n",
    "                types.add(disease_type)\n",
    "                found = True\n",
    "        if not found:\n",
    "            types.add('Other')  # Assign 'Other' if disease not found in mapping\n",
    "    disease_to_type[col] = types\n",
    "\n",
    "# Step 3: For each disease type, sum the corresponding disease columns\n",
    "for disease_type in disease_type_mapping.keys():\n",
    "    # Get all columns that map to this disease type\n",
    "    cols = [col for col, types in disease_to_type.items() if disease_type in types]\n",
    "    if cols:\n",
    "        # Sum the columns (since they are binary, this counts the number of diseases)\n",
    "        df[disease_type] = df[cols].sum(axis=1)\n",
    "    else:\n",
    "        df[disease_type] = 0  # If no diseases of this type are present\n",
    "\n",
    "# Step 4: Optionally, drop the individual disease columns\n",
    "df.drop(columns=disease_columns, inplace=True)\n",
    "\n",
    "df.to_csv('improving2.csv', index=False)\n",
    "\n",
    "# Now, df contains the non-disease columns and the summed disease type columns\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testando Melhores Variaveis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mensemble\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GradientBoostingClassifier\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Update the classifier in the pipeline to Gradient Boosting\u001b[39;00m\n\u001b[0;32m      4\u001b[0m model_pipeline \u001b[38;5;241m=\u001b[39m Pipeline(steps\u001b[38;5;241m=\u001b[39m[\n\u001b[0;32m      5\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpreprocessor\u001b[39m\u001b[38;5;124m'\u001b[39m, preprocessor),\n\u001b[0;32m      6\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclassifier\u001b[39m\u001b[38;5;124m'\u001b[39m, GradientBoostingClassifier(random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m))\n\u001b[0;32m      7\u001b[0m ])\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# Update the classifier in the pipeline to Gradient Boosting\n",
    "model_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', GradientBoostingClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "# Define parameter grid for Gradient Boosting tuning\n",
    "param_grid = {\n",
    "    'classifier__n_estimators': [100, 200],\n",
    "    'classifier__max_depth': [3, 5, 10],\n",
    "    'classifier__learning_rate': [0.01, 0.1, 0.2]\n",
    "}\n",
    "\n",
    "# Use GridSearchCV for hyperparameter tuning\n",
    "grid_search = GridSearchCV(model_pipeline, param_grid, cv=3, n_jobs=-1, verbose=2)\n",
    "\n",
    "# Fit the model\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = grid_search.best_estimator_.predict(X_test)\n",
    "print('Classification Report:\\n', classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'antibiotic_resistance_model.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 6\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjoblib\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Load the model later\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m loaded_model \u001b[38;5;241m=\u001b[39m \u001b[43mjoblib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mantibiotic_resistance_model.pkl\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;124;03m# Assuming new_data is a DataFrame with the same structure as your original X\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;124;03mnew_predictions = loaded_model.predict(new_data)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;124;03mprint(new_predictions)\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m     15\u001b[0m X_train\u001b[38;5;241m.\u001b[39mcolumns\n",
      "File \u001b[1;32mc:\\Users\\bitan\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\joblib\\numpy_pickle.py:650\u001b[0m, in \u001b[0;36mload\u001b[1;34m(filename, mmap_mode)\u001b[0m\n\u001b[0;32m    648\u001b[0m         obj \u001b[38;5;241m=\u001b[39m _unpickle(fobj)\n\u001b[0;32m    649\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 650\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m    651\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m _read_fileobject(f, filename, mmap_mode) \u001b[38;5;28;01mas\u001b[39;00m fobj:\n\u001b[0;32m    652\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(fobj, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    653\u001b[0m                 \u001b[38;5;66;03m# if the returned file object is a string, this means we\u001b[39;00m\n\u001b[0;32m    654\u001b[0m                 \u001b[38;5;66;03m# try to load a pickle file generated with an version of\u001b[39;00m\n\u001b[0;32m    655\u001b[0m                 \u001b[38;5;66;03m# Joblib so we load it with joblib compatibility function.\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'antibiotic_resistance_model.pkl'"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "\n",
    "\n",
    "# Load the model later\n",
    "loaded_model = joblib.load('antibiotic_resistance_model.pkl')\n",
    "\n",
    "'''\n",
    "# Assuming new_data is a DataFrame with the same structure as your original X\n",
    "new_predictions = loaded_model.predict(new_data)\n",
    "\n",
    "# View the predictions\n",
    "print(new_predictions)\n",
    "'''\n",
    "X_train.columns\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
